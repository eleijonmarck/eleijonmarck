import { h as createVNode, F as Fragment } from './astro.98e1a385.mjs';
import 'html-escaper';
import 'cookie';
import 'kleur/colors';
import 'slash';
import 'path-to-regexp';
import 'mime';
import 'string-width';

const html = "<p><img src=\"/media/linear_regression_error.gif\" alt=\"linear-regression.gif\"></p>\n<p>Gradient descent is one of those “greatest hits” algorithms that can offer a new perspective for solving problems. Unfortunately, it’s rarely taught in undergraduate computer science programs. In this post I’ll give an introduction to the gradient descent algorithm, and walk through an example that demonstrates how gradient descent can be used to solve machine learning problems such as linear regression.</p>\n<p>Gradient descent is <em><strong>widely</strong></em> used in Machine Learning and Deep Learning</p>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #81A1C1\">import</span><span style=\"color: #D8DEE9FF\"> pandas </span><span style=\"color: #81A1C1\">as</span><span style=\"color: #D8DEE9FF\"> pd</span></span>\n<span class=\"line\"><span style=\"color: #81A1C1\">import</span><span style=\"color: #D8DEE9FF\"> numpy </span><span style=\"color: #81A1C1\">as</span><span style=\"color: #D8DEE9FF\"> np</span></span>\n<span class=\"line\"><span style=\"color: #81A1C1\">import</span><span style=\"color: #D8DEE9FF\"> altair </span><span style=\"color: #81A1C1\">as</span><span style=\"color: #D8DEE9FF\"> alt</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">data </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> pd</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">read_csv</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">demo.txt</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">)</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #D8DEE9FF\">scatter </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> alt</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">Chart</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">data</span><span style=\"color: #ECEFF4\">).</span><span style=\"color: #88C0D0\">mark_circle</span><span style=\"color: #ECEFF4\">().</span><span style=\"color: #88C0D0\">encode</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #D8DEE9\">x</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">x:Q</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #D8DEE9\">y</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">y:Q</span><span style=\"color: #ECEFF4\">'</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">).</span><span style=\"color: #88C0D0\">properties</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #D8DEE9\">title</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">data</span><span style=\"color: #ECEFF4\">'</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">scatter</span></span></code></pre>\n<p><img src=\"./images/gradient-descent/output_4_0.png\" alt=\"png\"></p>\n<h1 id=\"our-goal-is-to-align-a-line-to-this-dataset\">Our goal is to align a line to this dataset</h1>\n<ul>\n<li>why would we want to do that?</li>\n<li>we can use this to infer properties of the dataset</li>\n<li>we can use it to predict future behaviour (extrapolate)</li>\n</ul>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #81A1C1\">from</span><span style=\"color: #D8DEE9FF\"> sklearn </span><span style=\"color: #81A1C1\">import</span><span style=\"color: #D8DEE9FF\"> linear_model</span></span>\n<span class=\"line\"><span style=\"color: #81A1C1\">from</span><span style=\"color: #D8DEE9FF\"> sklearn </span><span style=\"color: #81A1C1\">import</span><span style=\"color: #D8DEE9FF\"> model_selection</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">model </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> linear_model</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">LinearRegression</span><span style=\"color: #ECEFF4\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">X_train</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> X_test</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> y_train</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> y_test </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> model_selection</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">train_test_split</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">data</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">x</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">],</span><span style=\"color: #D8DEE9FF\"> data</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">y</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">],</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">test_size</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #B48EAD\">0.2</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">random_state</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #B48EAD\">0</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">model</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">fit</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">X_train</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">values</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">reshape</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #81A1C1\">-</span><span style=\"color: #B48EAD\">1</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">1</span><span style=\"color: #ECEFF4\">),</span><span style=\"color: #D8DEE9FF\"> y_train</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #616E88\">#For retrieving the slope:</span></span>\n<span class=\"line\"><span style=\"color: #88C0D0\">print</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #ECEFF4\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color: #A3BE8C\">Model intercept (position of the line) </span><span style=\"color: #EBCB8B\">\\n{</span><span style=\"color: #81A1C1\">:.2f</span><span style=\"color: #EBCB8B\">}</span></span>\n<span class=\"line\"><span style=\"color: #A3BE8C\">Model coefficients (slope of the line) </span><span style=\"color: #EBCB8B\">\\n{</span><span style=\"color: #81A1C1\">:.2f</span><span style=\"color: #EBCB8B\">}</span></span>\n<span class=\"line\"><span style=\"color: #A3BE8C\">Model score (how close are we to fit a line to the data) </span><span style=\"color: #EBCB8B\">\\n{</span><span style=\"color: #81A1C1\">:.2f</span><span style=\"color: #EBCB8B\">}</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">\"\"\"</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">format</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    model</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">intercept_</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    model</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">coef_</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #B48EAD\">0</span><span style=\"color: #ECEFF4\">],</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    model</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">score</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">X_test</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">values</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">reshape</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #81A1C1\">-</span><span style=\"color: #B48EAD\">1</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">1</span><span style=\"color: #ECEFF4\">),</span><span style=\"color: #D8DEE9FF\"> y_test</span><span style=\"color: #ECEFF4\">)))</span></span></code></pre>\n<p>This gives us the intercept $b$ 6.69 (where the line should be centered around) and the coefficient of $m$ 1.35.\nWhich given our modeled loss function is a score of 0.27</p>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #81A1C1\">def</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">make_line_using</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9\">m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">b</span><span style=\"color: #ECEFF4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #616E88\"># y = m * x + b</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    x </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> np</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">arange</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #B48EAD\">100</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    y </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> m </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> x </span><span style=\"color: #81A1C1\">+</span><span style=\"color: #D8DEE9FF\"> b</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    df </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> pd</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">DataFrame</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">np</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">matrix</span><span style=\"color: #ECEFF4\">([</span><span style=\"color: #D8DEE9FF\">x</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\">y</span><span style=\"color: #ECEFF4\">]).</span><span style=\"color: #D8DEE9FF\">T</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">columns</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">x</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">y</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">])</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    line </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> alt</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">Chart</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">df</span><span style=\"color: #ECEFF4\">).</span><span style=\"color: #88C0D0\">mark_line</span><span style=\"color: #ECEFF4\">().</span><span style=\"color: #88C0D0\">encode</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        </span><span style=\"color: #D8DEE9\">x</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">x:Q</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        </span><span style=\"color: #D8DEE9\">y</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">y:Q</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        </span><span style=\"color: #D8DEE9\">tooltip</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #D8DEE9FF\">alt</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">Tooltip</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">y</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">title</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">b * x + m</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">)]</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #ECEFF4\">).</span><span style=\"color: #88C0D0\">interactive</span><span style=\"color: #ECEFF4\">()</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #81A1C1\">return</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">scatter </span><span style=\"color: #81A1C1\">+</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">line</span><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">m_guess </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> model</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">coef_</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #B48EAD\">0</span><span style=\"color: #ECEFF4\">]</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">b_guess </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> model</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">intercept_</span></span>\n<span class=\"line\"><span style=\"color: #88C0D0\">make_line_using</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">m_guess</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b_guess</span><span style=\"color: #ECEFF4\">)</span></span></code></pre>\n<p><img src=\"./images/gradient-descent/output_7_0.png\" alt=\"png\"></p>\n<h1 id=\"now-lets-try-to-implement-this-ourselves\">Now let’s try to implement this ourselves!</h1>\n<h3 id=\"naive-approach-to-guess-until-we-get-a-good-fit\">Naive approach to guess until we get a good fit</h3>\n<p>guessing the beta parameters linear equation</p>\n<p>$ y = m \\times \\mathbf{x} + b$</p>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #81A1C1\">def</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">plot_on_top_of_data</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9\">m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">b</span><span style=\"color: #ECEFF4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #616E88\"># y = B_2 * x + B_1</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    x </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> np</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">arange</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #B48EAD\">100</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    y </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> b </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> x </span><span style=\"color: #81A1C1\">+</span><span style=\"color: #D8DEE9FF\"> m</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    df </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> pd</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">DataFrame</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">np</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">matrix</span><span style=\"color: #ECEFF4\">([</span><span style=\"color: #D8DEE9FF\">x</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\">y</span><span style=\"color: #ECEFF4\">]).</span><span style=\"color: #D8DEE9FF\">T</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">columns</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">x</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">y</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">])</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    line </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> alt</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">Chart</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">df</span><span style=\"color: #ECEFF4\">).</span><span style=\"color: #88C0D0\">mark_line</span><span style=\"color: #ECEFF4\">().</span><span style=\"color: #88C0D0\">encode</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        </span><span style=\"color: #D8DEE9\">x</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">x:Q</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        </span><span style=\"color: #D8DEE9\">y</span><span style=\"color: #81A1C1\">=</span><span style=\"color: #ECEFF4\">'</span><span style=\"color: #A3BE8C\">y:Q</span><span style=\"color: #ECEFF4\">'</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #81A1C1\">return</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">scatter </span><span style=\"color: #81A1C1\">+</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">line</span><span style=\"color: #ECEFF4\">))</span></span></code></pre>\n<p>Our guess</p>\n<p>$ y = 2 \\times \\mathbf{x} + 1$</p>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #D8DEE9FF\">m_guess </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">2</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">b_guess </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">1</span></span>\n<span class=\"line\"><span style=\"color: #88C0D0\">plot_on_top_of_data</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">m_guess</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b_guess</span><span style=\"color: #ECEFF4\">)</span></span></code></pre>\n<p><img src=\"./images/gradient-descent/output_11_0.png\" alt=\"png\"></p>\n<h4 id=\"not-the-most-sufficient-algorithm-but-might-work\">Not the most sufficient algorithm but might work.</h4>\n<p>hmmmmmm ¯\\<em>(ツ)</em>/¯\nlet’s think of another approach.</p>\n<blockquote>\n<p>Can we we somehow see if we have a good guess?</p>\n</blockquote>\n<h1 id=\"lets-improve-our-guessing-strategy-using-gradient-descent\">Let’s improve our guessing strategy using Gradient Descent</h1>\n<p><strong>Gradient descent</strong> is an optimization algorithm used to <strong>minimize some function (loss function)</strong> by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.</p>\n<p><img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent.png\" alt=\"landscape\"></p>\n<p>Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient.\nWe continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.</p>\n<p><img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png\" alt=\"winner\"></p>\n<p><a href=\"https://math.stackexchange.com/a/1695446/196117\" target=\"_blank\" rel=\"nofollow\">math.stackexchange - Partial derivative in gradient descent</a></p>\n<p><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html\" target=\"_blank\" rel=\"nofollow\">ml-cheatsheet</a></p>\n<h2 id=\"lets-introduce-the-loss-function-or-costerror\">Let’s introduce the loss function (or cost/error)</h2>\n<p>A Loss Functions tells us “how good” our model is at making predictions for a given set of parameters. The loss function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.</p>\n<p>${\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum <em>{i=1}^{n}(Y</em>{i}-{\\hat {Y_{i}}})^{2}.}$</p>\n<p>Given ${\\displaystyle n}$ predictions generated to ${\\hat{Y}}$, and ${\\displaystyle Y}$ is the vector of observed values of the variable being predicted.</p>\n<hr>\n<p>Our example with\n$ \\hat{Y} = mx_i + b$</p>\n<p>Now let’s run gradient descent using our new loss function. There are two parameters in our lost function we can control: m (weight) and b (bias).</p>\n<p>Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the loss function with respect to each parameter and store the results in a gradient.</p>\n<p>Given the loss function:</p>\n<p>$$\nf(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\n$$</p>\n<p>The gradient can be calculated as:</p>\n<h1 id=\"fmb-beginbmatrixfracdfdmfracdfdbendbmatrix\">$$\nf’(m,b) =\n\\begin{bmatrix}\n\\frac{df}{dm}\\\n\\frac{df}{db}\\\n\\end{bmatrix}</h1>\n<p>\\begin{bmatrix}\n\\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\n\\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\n\\end{bmatrix}\n$$</p>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #81A1C1\">def</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">step_gradient</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9\">m</span><span style=\"color: #ECEFF4\">:</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">int</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">b</span><span style=\"color: #ECEFF4\">:</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">int</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">points</span><span style=\"color: #ECEFF4\">:</span><span style=\"color: #D8DEE9FF\"> np</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">ndarray</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">learning_rate</span><span style=\"color: #ECEFF4\">:</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">float</span><span style=\"color: #ECEFF4\">)</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">-></span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">list</span><span style=\"color: #ECEFF4\">:</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #ECEFF4\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color: #A3BE8C\">    this calculates the gradient step of a **linear function**</span></span>\n<span class=\"line\"><span style=\"color: #A3BE8C\">    WILL NOT WORK for multiple dimensional data,</span></span>\n<span class=\"line\"><span style=\"color: #A3BE8C\">    since the derivates will be on matricies instead</span></span>\n<span class=\"line\"><span style=\"color: #A3BE8C\">    </span><span style=\"color: #ECEFF4\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    b_gradient </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">0</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    m_gradient </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">0</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    N </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">float</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #88C0D0\">len</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">points</span><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #81A1C1\">for</span><span style=\"color: #D8DEE9FF\"> i </span><span style=\"color: #81A1C1\">in</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">range</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #B48EAD\">0</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">len</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">points</span><span style=\"color: #ECEFF4\">)):</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        x </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> points</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #D8DEE9FF\">i</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">0</span><span style=\"color: #ECEFF4\">]</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        y </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> points</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #D8DEE9FF\">i</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">1</span><span style=\"color: #ECEFF4\">]</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        b_gradient </span><span style=\"color: #81A1C1\">+=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #81A1C1\">-</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #B48EAD\">2</span><span style=\"color: #81A1C1\">/</span><span style=\"color: #D8DEE9FF\">N</span><span style=\"color: #ECEFF4\">)</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">y </span><span style=\"color: #81A1C1\">-</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">((</span><span style=\"color: #D8DEE9FF\">m </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> x</span><span style=\"color: #ECEFF4\">)</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #81A1C1\">+</span><span style=\"color: #D8DEE9FF\"> b</span><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        m_gradient </span><span style=\"color: #81A1C1\">+=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #81A1C1\">-</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #B48EAD\">2</span><span style=\"color: #81A1C1\">/</span><span style=\"color: #D8DEE9FF\">N</span><span style=\"color: #ECEFF4\">)</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> x </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">y </span><span style=\"color: #81A1C1\">-</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">((</span><span style=\"color: #D8DEE9FF\">m </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> x</span><span style=\"color: #ECEFF4\">)</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #81A1C1\">+</span><span style=\"color: #D8DEE9FF\"> b</span><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    b </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> b </span><span style=\"color: #81A1C1\">-</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">learning_rate </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> b_gradient</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    m </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> m </span><span style=\"color: #81A1C1\">-</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">learning_rate </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> m_gradient</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #81A1C1\">return</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #D8DEE9FF\">b</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> m</span><span style=\"color: #ECEFF4\">]</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #D8DEE9FF\">new_b</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> new_m </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">step_gradient</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">m_guess</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b_guess</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> data</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">values</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> learning_rate</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">new_b</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> new_m</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #d8dee9ff\">(6.6874785109966535, 1.3465666635682905)</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #616E88\"># Let's run through it through the whole dataset</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #81A1C1\">def</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">gradient_descent_runner</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9\">points</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">starting_m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">starting_b</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">learning_rate</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">num_iterations</span><span style=\"color: #ECEFF4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    m </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> starting_m</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    b </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> starting_b</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #81A1C1\">for</span><span style=\"color: #D8DEE9FF\"> i </span><span style=\"color: #81A1C1\">in</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">range</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">num_iterations</span><span style=\"color: #ECEFF4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">step_gradient</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> np</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">array</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">points</span><span style=\"color: #ECEFF4\">),</span><span style=\"color: #D8DEE9FF\"> learning_rate</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #81A1C1\">return</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #D8DEE9FF\">m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b</span><span style=\"color: #ECEFF4\">]</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #D8DEE9FF\">number_iterations </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">10000</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">[</span><span style=\"color: #D8DEE9FF\">m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b</span><span style=\"color: #ECEFF4\">]</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">gradient_descent_runner</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    data</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    m_guess</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    b_guess</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    learning_rate</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    number_iterations</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">)</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #88C0D0\">print</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #ECEFF4\">\"</span><span style=\"color: #A3BE8C\">Starting gradient descent at guess_m = </span><span style=\"color: #EBCB8B\">{0}</span><span style=\"color: #A3BE8C\">, guess_b = </span><span style=\"color: #EBCB8B\">{1}</span><span style=\"color: #ECEFF4\">\"</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">format</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    m_guess</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    b_guess</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"><span style=\"color: #88C0D0\">print</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #ECEFF4\">\"</span><span style=\"color: #A3BE8C\">Last gradient descent at guess_m = </span><span style=\"color: #EBCB8B\">{0}</span><span style=\"color: #A3BE8C\">, guess_b = </span><span style=\"color: #EBCB8B\">{1}</span><span style=\"color: #ECEFF4\">\"</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">format</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    m</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    b</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #d8dee9ff\">Starting gradient descent at guess_m = 1.3450919020620442, guess_b = 6.687439682550092</span></span>\n<span class=\"line\"><span style=\"color: #d8dee9ff\">Last gradient descent at guess_m = 1.4510680203998683, guess_b = 1.4510195909326549</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #81A1C1\">def</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">compute_error_for_line</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9\">m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">b</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #D8DEE9\">points</span><span style=\"color: #ECEFF4\">):</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    total_error </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">0</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #616E88\"># sum (y_i - y_hat_i) ^ 2</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #81A1C1\">for</span><span style=\"color: #D8DEE9FF\"> i </span><span style=\"color: #81A1C1\">in</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">range</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #B48EAD\">0</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">len</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">points</span><span style=\"color: #ECEFF4\">)):</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        x </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> points</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #D8DEE9FF\">i</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">0</span><span style=\"color: #ECEFF4\">]</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        y </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> points</span><span style=\"color: #ECEFF4\">[</span><span style=\"color: #D8DEE9FF\">i</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">1</span><span style=\"color: #ECEFF4\">]</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">        total_error </span><span style=\"color: #81A1C1\">+=</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">y </span><span style=\"color: #81A1C1\">-</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">m </span><span style=\"color: #81A1C1\">*</span><span style=\"color: #D8DEE9FF\"> x </span><span style=\"color: #81A1C1\">+</span><span style=\"color: #D8DEE9FF\"> b</span><span style=\"color: #ECEFF4\">))</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #81A1C1\">**</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #B48EAD\">2</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #616E88\"># 1 / n</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    mse </span><span style=\"color: #81A1C1\">=</span><span style=\"color: #D8DEE9FF\"> total_error </span><span style=\"color: #81A1C1\">/</span><span style=\"color: #D8DEE9FF\"> </span><span style=\"color: #88C0D0\">float</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #88C0D0\">len</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">points</span><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #81A1C1\">return</span><span style=\"color: #D8DEE9FF\"> mse</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #616E88\"># lets see how bad our guess was</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #88C0D0\">compute_error_for_line</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">m_guess</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b_guess</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> data</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">values</span><span style=\"color: #ECEFF4\">)</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #d8dee9ff\">838.9099083602013</span></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #88C0D0\">print</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #ECEFF4\">\"</span><span style=\"color: #A3BE8C\">Starting gradient descent at </span><span style=\"color: #EBCB8B\">\\n</span><span style=\"color: #A3BE8C\"> guess_m = </span><span style=\"color: #EBCB8B\">{0}</span><span style=\"color: #A3BE8C\">, guess_b = </span><span style=\"color: #EBCB8B\">{1}</span><span style=\"color: #A3BE8C\">, error </span><span style=\"color: #EBCB8B\">{2}</span><span style=\"color: #ECEFF4\">\"</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">format</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    m_guess</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    b_guess</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #88C0D0\">compute_error_for_line</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">m_guess</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b_guess</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> data</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">values</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"><span style=\"color: #88C0D0\">print</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #ECEFF4\">\"</span><span style=\"color: #A3BE8C\">Last gradient descent at </span><span style=\"color: #EBCB8B\">\\n</span><span style=\"color: #A3BE8C\"> guess_m = </span><span style=\"color: #EBCB8B\">{0}</span><span style=\"color: #A3BE8C\">, guess_b = </span><span style=\"color: #EBCB8B\">{1}</span><span style=\"color: #A3BE8C\">, error </span><span style=\"color: #EBCB8B\">{2}</span><span style=\"color: #ECEFF4\">\"</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #88C0D0\">format</span><span style=\"color: #ECEFF4\">(</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    m</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    b</span><span style=\"color: #ECEFF4\">,</span></span>\n<span class=\"line\"><span style=\"color: #D8DEE9FF\">    </span><span style=\"color: #88C0D0\">compute_error_for_line</span><span style=\"color: #ECEFF4\">(</span><span style=\"color: #D8DEE9FF\">m</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> b</span><span style=\"color: #ECEFF4\">,</span><span style=\"color: #D8DEE9FF\"> data</span><span style=\"color: #ECEFF4\">.</span><span style=\"color: #D8DEE9FF\">values</span><span style=\"color: #ECEFF4\">)</span></span>\n<span class=\"line\"><span style=\"color: #ECEFF4\">))</span></span>\n<span class=\"line\"></span></code></pre>\n<pre is:raw=\"\" class=\"astro-code\" style=\"background-color: #2e3440ff; overflow-x: auto;\"><code><span class=\"line\"><span style=\"color: #d8dee9ff\">Starting gradient descent at</span></span>\n<span class=\"line\"><span style=\"color: #d8dee9ff\"> guess_m = 2, guess_b = 1, error 838.9099083602013</span></span>\n<span class=\"line\"><span style=\"color: #d8dee9ff\">Last gradient descent at</span></span>\n<span class=\"line\"><span style=\"color: #d8dee9ff\"> guess_m = 1.4510680203998683, guess_b = 1.4510195909326549, error 111.87217648730648</span></span></code></pre>\n<h1 id=\"how-does-gradient-descent-work\">How does gradient descent work?</h1>\n<hr>\n<p>In it’s most general form:</p>\n<p>Gradient descent is based on the observation that if the multi-variable function ${\\displaystyle F(\\mathbf {x} )}$ is defined and differentiable in a neighborhood of a point ${\\displaystyle \\mathbf {a} }$ , then ${\\displaystyle F(\\mathbf {x} )}$ decreases fastest if one goes from ${\\displaystyle \\mathbf {a} }$  in the direction of the negative gradient of ${\\displaystyle F}$ at ${\\displaystyle,-\\nabla F(\\mathbf {a} )}$. It follows that, if</p>\n<p>${\\displaystyle \\mathbf {a} _{n+1}=\\mathbf {a} _{n}-\\gamma \\nabla F(\\mathbf {a} _{n})}$</p>\n<p>for ${\\displaystyle \\gamma \\in \\mathbb {R} <em>{+}}$ small enough, then ${\\displaystyle F(\\mathbf {a</em>{n}} )\\geq F(\\mathbf {a_{n+1}} )}$. In other words, the term ${\\displaystyle \\gamma \\nabla F(\\mathbf {a} )}$ is subtracted from ${\\displaystyle \\mathbf {a} }$  because we want to move against the gradient, toward the minimum.</p>\n<p>Here we have defined and the algorith works with contraints:</p>\n<p><strong>learning rate</strong> ${\\gamma}$, for small ${\\displaystyle \\gamma \\in \\mathbb {R} _{+}}$</p>\n<p><strong>function</strong> $F(\\mathbf{x})$,  if differentiable; then ${\\displaystyle F(\\mathbf {a_{n}} )\\geq F(\\mathbf {a_{n+1}} )}$</p>\n<p><strong>Leading to</strong> ($\\leadsto $) a monotonic sequence $F(\\mathbf {x} _{0})\\geq F(\\mathbf {x} _{1})\\geq F(\\mathbf {x} _{2})\\geq \\cdots,$</p>\n<p><img src=\"/media/gradient-descent-cover.gif\" alt=\"gradient-descent-cover.gif\"></p>\n<h1 id=\"coming-back-to-the-real-world\">Coming back to the real world</h1>\n<p>Scikit learn provides you two approaches to linear regression:</p>\n<p>If you can decompose your loss function into additive terms, then stochastic approach is known to behave better (thus SGD) and if you can spare enough memory - OLS method is faster and easier (thus first solution).</p>\n<ol>\n<li>\n<p><code>LinearRegression</code> object uses <strong>Ordinary Least Squares</strong> solver from scipy, as LR is one of two classifiers which have closed form solution. Despite the ML course - you can actually learn this model by just inverting and multiplicating some matrices.</p>\n</li>\n<li>\n<p><code>SGDRegressor</code> which is an implementation of <strong>stochastic gradient descent</strong>, very generic one where you can choose your penalty terms. To obtain linear regression you choose loss to be L2 and penalty also to none (linear regression) or L2 (Ridge regression)</p>\n</li>\n</ol>\n<p>The “gradient descent” is a major part of most learning algorithm. What you will see most often is a improved version of the algorithm <strong>Stochastic Gradient Descent</strong>.</p>\n<p>source - <a href=\"https://stackoverflow.com/questions/34469237/linear-regression-and-gradient-descent-in-scikit-learn-pandas\" target=\"_blank\" rel=\"nofollow\">linear regression scitkit</a></p>";

				const frontmatter = {"title":"An Introduction to Gradient Descent w. Linear Regression","date":"2019-06-09T22:40:32.169Z","template":"post","draft":false,"slug":"understanding-gradient-descent","category":"algorithms","tags":["statistics","ml","optimization"],"description":"Gradient descent is one of 'greatest hits' algorithms. This posts gives a detailed explained and walkthrough of why and how it is implemented and applied to an example for linear regression","socialImage":"/media/gradient-descent-cover.gif"};
				const file = "/Users/eleijonmarck/dev/eleijonmarck/better-bar/src/data/blog-posts/2019-06-09---understanding-gradient-descent.md";
				const url = undefined;
				function rawContent() {
					return "\n![linear-regression.gif](/media/linear_regression_error.gif)\n\n\nGradient descent is one of those “greatest hits” algorithms that can offer a new perspective for solving problems. Unfortunately, it’s rarely taught in undergraduate computer science programs. In this post I’ll give an introduction to the gradient descent algorithm, and walk through an example that demonstrates how gradient descent can be used to solve machine learning problems such as linear regression.\n\nGradient descent is _**widely**_ used in Machine Learning and Deep Learning\n\n\n```python\nimport pandas as pd\nimport numpy as np\nimport altair as alt\ndata = pd.read_csv('demo.txt')\n```\n\n\n```python\nscatter = alt.Chart(data).mark_circle().encode(\n    x='x:Q',\n    y='y:Q'\n).properties(\n    title='data'\n)\nscatter\n```\n\n\n\n\n![png](./images/gradient-descent/output_4_0.png)\n\n\n\n# Our goal is to align a line to this dataset\n\n- why would we want to do that?\n - we can use this to infer properties of the dataset\n - we can use it to predict future behaviour (extrapolate)\n\n\n```python\nfrom sklearn import linear_model\nfrom sklearn import model_selection\nmodel = linear_model.LinearRegression()\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(data['x'], data['y'], test_size=0.2, random_state=0)\n\nmodel.fit(X_train.values.reshape(-1, 1), y_train)\n\n#For retrieving the slope:\nprint(\"\"\"\nModel intercept (position of the line) \\n{:.2f}\nModel coefficients (slope of the line) \\n{:.2f}\nModel score (how close are we to fit a line to the data) \\n{:.2f}\n\"\"\".format(\n    model.intercept_,\n    model.coef_[0],\n    model.score(X_test.values.reshape(-1, 1), y_test)))\n```\n\nThis gives us the intercept $b$ 6.69 (where the line should be centered around) and the coefficient of $m$ 1.35.\nWhich given our modeled loss function is a score of 0.27\n\n\n\n```python\ndef make_line_using(m, b):\n    # y = m * x + b\n    x = np.arange(100)\n    y = m * x + b\n    df = pd.DataFrame(np.matrix([x,y]).T, columns=['x','y'])\n    line = alt.Chart(df).mark_line().encode(\n        x='x:Q',\n        y='y:Q',\n        tooltip=[alt.Tooltip('y', title='b * x + m')]\n    ).interactive()\n    return (scatter + (line))\n\n\n\nm_guess = model.coef_[0]\nb_guess = model.intercept_\nmake_line_using(m_guess, b_guess)\n```\n\n\n\n\n![png](./images/gradient-descent/output_7_0.png)\n\n\n\n# Now let's try to implement this ourselves!\n\n### Naive approach to guess until we get a good fit\nguessing the beta parameters linear equation\n\n\n$ y = m \\times \\mathbf{x} + b$\n\n\n```python\ndef plot_on_top_of_data(m, b):\n    # y = B_2 * x + B_1\n    x = np.arange(100)\n    y = b * x + m\n    df = pd.DataFrame(np.matrix([x,y]).T, columns=['x','y'])\n    line = alt.Chart(df).mark_line().encode(\n        x='x:Q',\n        y='y:Q'\n    )\n    return (scatter + (line))\n```\n\nOur guess\n\n$ y = 2 \\times \\mathbf{x} + 1$\n\n\n```python\nm_guess = 2\nb_guess = 1\nplot_on_top_of_data(m_guess, b_guess)\n```\n\n\n\n\n![png](./images/gradient-descent/output_11_0.png)\n\n\n\n#### Not the most sufficient algorithm but might work.\n\nhmmmmmm ¯\\\\_(ツ)_/¯\nlet's think of another approach.\n\n> Can we we somehow see if we have a good guess?\n\n# Let's improve our guessing strategy using Gradient Descent\n\n**Gradient descent** is an optimization algorithm used to **minimize some function (loss function)** by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\n\n![landscape](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent.png)\n\n\n\nStarting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient.\nWe continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.\n\n![winner](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)\n\n[math.stackexchange - Partial derivative in gradient descent](https://math.stackexchange.com/a/1695446/196117)\n\n[ml-cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)\n\n## Let's introduce the loss function (or cost/error)\n\nA Loss Functions tells us “how good” our model is at making predictions for a given set of parameters. The loss function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.\n\n${\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.}$\n\n\n\nGiven ${\\displaystyle n}$ predictions generated to ${\\hat{Y}}$, and ${\\displaystyle Y}$ is the vector of observed values of the variable being predicted.\n\n---\nOur example with\n$ \\hat{Y} = mx_i + b$\n\nNow let’s run gradient descent using our new loss function. There are two parameters in our lost function we can control: m (weight) and b (bias).\n\nSince we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the loss function with respect to each parameter and store the results in a gradient.\n\n\n\nGiven the loss function:\n\n$$\nf(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\n$$\n\nThe gradient can be calculated as:\n\n$$\nf'(m,b) =\n   \\begin{bmatrix}\n     \\frac{df}{dm}\\\\\n     \\frac{df}{db}\\\\\n    \\end{bmatrix}\n=\n   \\begin{bmatrix}\n     \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\\n     \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\\n    \\end{bmatrix}\n$$\n\n\n```python\ndef step_gradient(m: int, b: int, points: np.ndarray, learning_rate: float) -> list:\n    \"\"\"\n    this calculates the gradient step of a **linear function**\n    WILL NOT WORK for multiple dimensional data,\n    since the derivates will be on matricies instead\n    \"\"\"\n    b_gradient = 0\n    m_gradient = 0\n    N = float(len(points))\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        b_gradient += -(2/N) * (y - ((m * x) + b))\n        m_gradient += -(2/N) * x * (y - ((m * x) + b))\n    b = b - (learning_rate * b_gradient)\n    m = m - (learning_rate * m_gradient)\n    return [b, m]\n```\n\n\n```python\nnew_b, new_m = step_gradient(m_guess, b_guess, data.values, learning_rate)\nnew_b, new_m\n```\n\n\n\n\n    (6.6874785109966535, 1.3465666635682905)\n\n\n\n\n```python\n# Let's run through it through the whole dataset\n```\n\n\n```python\ndef gradient_descent_runner(points, starting_m, starting_b, learning_rate, num_iterations):\n    m = starting_m\n    b = starting_b\n    for i in range(num_iterations):\n        m, b = step_gradient(m, b, np.array(points), learning_rate)\n    return [m, b]\n```\n\n\n```python\nnumber_iterations = 10000\n[m, b] = gradient_descent_runner(\n    data,\n    m_guess,\n    b_guess,\n    learning_rate,\n    number_iterations\n)\n```\n\n\n```python\nprint(\"Starting gradient descent at guess_m = {0}, guess_b = {1}\".format(\n    m_guess,\n    b_guess\n))\nprint(\"Last gradient descent at guess_m = {0}, guess_b = {1}\".format(\n    m,\n    b\n))\n\n```\n\n    Starting gradient descent at guess_m = 1.3450919020620442, guess_b = 6.687439682550092\n    Last gradient descent at guess_m = 1.4510680203998683, guess_b = 1.4510195909326549\n\n\n\n```python\ndef compute_error_for_line(m, b, points):\n    total_error = 0\n    # sum (y_i - y_hat_i) ^ 2\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        total_error += (y - (m * x + b)) ** 2\n    # 1 / n\n    mse = total_error / float(len(points))\n    return mse\n```\n\n\n```python\n# lets see how bad our guess was\n\ncompute_error_for_line(m_guess, b_guess, data.values)\n```\n\n\n\n\n    838.9099083602013\n\n\n\n\n```python\nprint(\"Starting gradient descent at \\n guess_m = {0}, guess_b = {1}, error {2}\".format(\n    m_guess,\n    b_guess,\n    compute_error_for_line(m_guess, b_guess, data.values)\n))\nprint(\"Last gradient descent at \\n guess_m = {0}, guess_b = {1}, error {2}\".format(\n    m,\n    b,\n    compute_error_for_line(m, b, data.values)\n))\n\n```\n\n    Starting gradient descent at\n     guess_m = 2, guess_b = 1, error 838.9099083602013\n    Last gradient descent at\n     guess_m = 1.4510680203998683, guess_b = 1.4510195909326549, error 111.87217648730648\n\n\n# How does gradient descent work?\n---\nIn it's most general form:\n\nGradient descent is based on the observation that if the multi-variable function ${\\displaystyle F(\\mathbf {x} )}$ is defined and differentiable in a neighborhood of a point ${\\displaystyle \\mathbf {a} }$ , then ${\\displaystyle F(\\mathbf {x} )}$ decreases fastest if one goes from ${\\displaystyle \\mathbf {a} }$  in the direction of the negative gradient of ${\\displaystyle F}$ at ${\\displaystyle,-\\nabla F(\\mathbf {a} )}$. It follows that, if\n\n\n\n${\\displaystyle \\mathbf {a} _{n+1}=\\mathbf {a} _{n}-\\gamma \\nabla F(\\mathbf {a} _{n})}$\n\nfor ${\\displaystyle \\gamma \\in \\mathbb {R} _{+}}$ small enough, then ${\\displaystyle F(\\mathbf {a_{n}} )\\geq F(\\mathbf {a_{n+1}} )}$. In other words, the term ${\\displaystyle \\gamma \\nabla F(\\mathbf {a} )}$ is subtracted from ${\\displaystyle \\mathbf {a} }$  because we want to move against the gradient, toward the minimum.\n\nHere we have defined and the algorith works with contraints:\n\n\n**learning rate** ${\\gamma}$, for small ${\\displaystyle \\gamma \\in \\mathbb {R} _{+}}$\n\n\n**function** $F(\\mathbf{x})$,  if differentiable; then ${\\displaystyle F(\\mathbf {a_{n}} )\\geq F(\\mathbf {a_{n+1}} )}$\n\n**Leading to** ($\\leadsto $) a monotonic sequence $F(\\mathbf {x} _{0})\\geq F(\\mathbf {x} _{1})\\geq F(\\mathbf {x} _{2})\\geq \\cdots,$\n\n![gradient-descent-cover.gif](/media/gradient-descent-cover.gif)\n\n# Coming back to the real world\n\nScikit learn provides you two approaches to linear regression:\n\nIf you can decompose your loss function into additive terms, then stochastic approach is known to behave better (thus SGD) and if you can spare enough memory - OLS method is faster and easier (thus first solution).\n\n1) `LinearRegression` object uses **Ordinary Least Squares** solver from scipy, as LR is one of two classifiers which have closed form solution. Despite the ML course - you can actually learn this model by just inverting and multiplicating some matrices.\n\n2) `SGDRegressor` which is an implementation of **stochastic gradient descent**, very generic one where you can choose your penalty terms. To obtain linear regression you choose loss to be L2 and penalty also to none (linear regression) or L2 (Ridge regression)\n\nThe \"gradient descent\" is a major part of most learning algorithm. What you will see most often is a improved version of the algorithm **Stochastic Gradient Descent**.\n\n\nsource - [linear regression scitkit](https://stackoverflow.com/questions/34469237/linear-regression-and-gradient-descent-in-scikit-learn-pandas)\n";
				}
				function compiledContent() {
					return html;
				}
				function getHeadings() {
					return [{"depth":1,"slug":"our-goal-is-to-align-a-line-to-this-dataset","text":"Our goal is to align a line to this dataset"},{"depth":1,"slug":"now-lets-try-to-implement-this-ourselves","text":"Now let’s try to implement this ourselves!"},{"depth":3,"slug":"naive-approach-to-guess-until-we-get-a-good-fit","text":"Naive approach to guess until we get a good fit"},{"depth":4,"slug":"not-the-most-sufficient-algorithm-but-might-work","text":"Not the most sufficient algorithm but might work."},{"depth":1,"slug":"lets-improve-our-guessing-strategy-using-gradient-descent","text":"Let’s improve our guessing strategy using Gradient Descent"},{"depth":2,"slug":"lets-introduce-the-loss-function-or-costerror","text":"Let’s introduce the loss function (or cost/error)"},{"depth":1,"slug":"fmb-beginbmatrixfracdfdmfracdfdbendbmatrix","text":"$$\nf’(m,b) =\n\\begin${bmatrix}\n\\frac${df}${dm}\\\n\\frac${df}${db}\\\n\\end${bmatrix}"},{"depth":1,"slug":"how-does-gradient-descent-work","text":"How does gradient descent work?"},{"depth":1,"slug":"coming-back-to-the-real-world","text":"Coming back to the real world"}];
				}
				async function Content() {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;
					const contentFragment = createVNode(Fragment, { 'set:html': html });
					return contentFragment;
				}
				Content[Symbol.for('astro.needsHeadRendering')] = true;

export { Content, compiledContent, Content as default, file, frontmatter, getHeadings, rawContent, url };
