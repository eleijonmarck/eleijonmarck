<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="lang">
  <head>
    <title data-vue-tag="true">An Introduction to Gradient Descent w. Linear Regression - #TODO: A*.find(information)</title><meta data-vue-tag="true" charset="utf-8"><meta data-vue-tag="true" name="generator" content="Gridsome v0.5.8"><meta data-vue-tag="true" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="true" data-key="description" name="description" content="A way to find some information"><meta data-vue-tag="true" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="true" name="description" content=""><link data-vue-tag="true" rel="icon" type="image/png" sizes="16x16" href="eleijonmarck/assets/static/favicon.ce0531f.9bb7ffa.png"><link data-vue-tag="true" rel="icon" type="image/png" sizes="32x32" href="eleijonmarck/assets/static/favicon.ac8d93a.9bb7ffa.png"><link data-vue-tag="true" rel="icon" type="image/png" sizes="96x96" href="eleijonmarck/assets/static/favicon.b9532cc.9bb7ffa.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="76x76" href="eleijonmarck/assets/static/favicon.f22e9f3.9bb7ffa.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="152x152" href="eleijonmarck/assets/static/favicon.62d22cb.9bb7ffa.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="120x120" href="eleijonmarck/assets/static/favicon.1539b60.9bb7ffa.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="167x167" href="eleijonmarck/assets/static/favicon.dc0cdc5.9bb7ffa.png"><link data-vue-tag="true" rel="apple-touch-icon" type="image/png" sizes="180x180" href="eleijonmarck/assets/static/favicon.7b22250.9bb7ffa.png"><noscript data-vue-tag="true" ><style>.g-image--loading{display:none;}</style></noscript><link rel="preload" href="eleijonmarck/assets/css/0.styles.04accc87.css" as="style"><link rel="preload" href="eleijonmarck/assets/js/app.f762706c.js" as="script"><link rel="preload" href="eleijonmarck/assets/js/component--post.60537bb2.js" as="script"><link rel="prefetch" href="eleijonmarck/assets/js/component--404.a7c2a474.js"><link rel="prefetch" href="eleijonmarck/assets/js/component--home.4e934f14.js"><link rel="prefetch" href="eleijonmarck/assets/js/component--tag.d54f72b5.js"><link rel="prefetch" href="eleijonmarck/assets/js/page-query.a8e7ed82.js"><link rel="stylesheet" href="eleijonmarck/assets/css/0.styles.04accc87.css">
  </head>
  <body data-vue-tag="">
    <script>
      // Add dark / light detection that runs before Vue.js load. Borrowed from overreacted.io
      (function() {
        window.__onThemeChange = function() {};
        function setTheme(newTheme) {
          window.__theme = newTheme;
          preferredTheme = newTheme;
          document.body.setAttribute('data-theme', newTheme);
          window.__onThemeChange(newTheme);
        }

        var preferredTheme;
        try {
          preferredTheme = localStorage.getItem('theme');
        } catch (err) { }

        window.__setPreferredTheme = function(newTheme) {
          setTheme(newTheme);
          try {
            localStorage.setItem('theme', newTheme);
          } catch (err) {}
        }

        var darkQuery = window.matchMedia('(prefers-color-scheme: dark)');
        darkQuery.addListener(function(e) {
          window.__setPreferredTheme(e.matches ? 'dark' : 'light')
        });

        setTheme(preferredTheme || (darkQuery.matches ? 'dark' : 'light'));
      })();
    </script>

    <div id="app" data-server-rendered="true"><header class="header"><div class="header__left"><a href="/eleijonmarck/" class="logo active"><span class="logo__text">
    ← #TODO: A*.find(information)
  </span></a></div><div class="header__right"><button role="button" aria-label="Toggle dark/light" class="toggle-theme"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></button></div></header><main class="main"><div class="post-title"><h1 class="post-title__text">
      An Introduction to Gradient Descent w. Linear Regression
    </h1><div class="post-meta">
   Posted 9. June 2019.
   <strong>7 min read.</strong></div></div><div class="post content-box"><div class="post__header"><!----></div><div class="post__content"><h1 id="an-introduction-to-gradient-descent-w-linear-regression"><a href="#an-introduction-to-gradient-descent-w-linear-regression" aria-hidden="true"><span class="icon icon-link"></span></a>An Introduction to Gradient Descent w. Linear Regression</h1>
<p>Gradient descent is one of those “greatest hits” algorithms that can offer a new perspective for solving problems. Unfortunately, it’s rarely taught in undergraduate computer science programs. In this post I’ll give an introduction to the gradient descent algorithm, and walk through an example that demonstrates how gradient descent can be used to solve machine learning problems such as linear regression.</p>
<p>Gradient descent is <em><strong>widely</strong></em> used in Machine Learning and Deep Learning</p>
<pre class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> altair <span class="token keyword">as</span> alt</pre>
<pre class="language-python"><span class="token comment">###########</span>
<span class="token comment"># actual data</span>
<span class="token comment">###########</span>

data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'demo.txt'</span><span class="token punctuation">)</span></pre>
<pre class="language-python">scatter <span class="token operator">=</span> alt<span class="token punctuation">.</span>Chart<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">.</span>mark_circle<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span>
    x<span class="token operator">=</span><span class="token string">'x:Q'</span><span class="token punctuation">,</span>
    y<span class="token operator">=</span><span class="token string">'y:Q'</span>
<span class="token punctuation">)</span><span class="token punctuation">.</span>properties<span class="token punctuation">(</span>
    title<span class="token operator">=</span><span class="token string">'data'</span>
<span class="token punctuation">)</span>
scatter</pre>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 450 364' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-0'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-0)' width='450' height='364' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAA0CAYAAAA62j4JAAAACXBIWXMAAAsSAAALEgHS3X78AAAQX0lEQVRo3tVbC3hUxfW/dzeP3Q1sdpNsNpD3ZpMACY8EMFAekoIIiWgFW7WKKBUQpUIDJCRCIOEZIIi8hFZAHnmYBASCgKKClFoRBQqBPBaDSdVECvzBaq2133//vzPMrDfr3nzhEfF/v%2b98c3fOPXPOnDlzHjOJJP28HvnOcpcZfzkoKEg%2bduyY9Mknn0h1dXWsJTh//rzr3R0EztM3nvqWLFnCeCYkJGjQ5AJWgP9CwMT4%2bHiN0%2bmU1qxZ06axqK/OcV76xxeN0uvvfCRt2/tX6VJzI%2bvzRKd8/7k8sYCuvLXdKSFoJSIAfcxmc1R1dbURq%2b/vcDj8oSkXQHvUmnjr79Ya0Zr4t0Y3OvptUtLQ2I2NjUastjcHH4KGhgaj4Cu%2b5TxbjMn5MRxW2//vDReM%2b46cCCp/68Pgps8%2bNZ6qqjFxeYyKcZgs/LehpQY0mkg0g7t3755QWlqacvHixU61tbXh%2bDiMA72HAgYCIjCA6A/DO%2bFC0PYFdAV04t8LOsL1B8TgvTOnYbgTJ050rqqqSjp79mzSqVOnGJ1i7M5EQ7Q0hnJM4kG88N63ptYR4nR%2bF1iw9d1fzt7w5hin85vAf135gmgHkMxcPpesAJKhp0dTwCpoc3Nzw9VMBYSRreBISIMKjoTQquDMBCo4LdF6wp2rqTP889LnQVg%2bbeoftg9RurRhGUWDq6pro9VkhRJaziMmJoa1c%2bfO1RcUFLC9WFlZqQFzmQAEMhcoloTijkTmeI1ikib%2bLugEzgbQq%2bCsBFwwF0%2bO0xOtCs706YV6ppynFr3u73ReoQWUJhTs0o6dvzO0ts5BfkVCK7vzRNvS3yxYsIC1ixcv1sFLM82hlRUrIVq7YsKiTwhEJubPGQhNC1w03nVudOKbYAIVnI5olWMpxqY9HSZkTJ9VKj0%2bf4drThcu1LNVrXM4lCvvkqeFAiIjr1sEzN9lATt37iStSRwEoR2DaBX9klAI319CAbKSjjQO0Ctxigm5FOABpxerxVdRqq93TYKcGbOAQx%2bc1qBfgkVIB/58kn5rz1bXxRHu88ZPZQqLynGFVXn0AdgG4W3eOy1xqj6AK0ergjMTqOC0YpLuT12dw3DmXG0nNXmeLtj1iNQ506ctvkxGFKCBEpOSkmK2bNmSfO3atYCamhqrWB0OQSBMoYkCLIp%2b%2bo4cWS/utQPccPS7D1dCkNuYNPluHMzu/DgN0QaQnyC%2bDsd56yu7/hKAcBf1/VfNFHVMcIghACv2e/DXlz8PcP73csDxv50biD4LLCMYtMpxAwGJbomgTPtlRGxsbK/i4uKky5cv/%2bQKoEhAfSQsFziIO9Y%2bfAwrPLvV6fza//H5O59aUXw4dW3ZkV%2bhLxATNju/v2xy/u8V07Lth5Jy1h9IgyJ6kgLcJu9RAXd8C9xIGAwes6bj0IyizCcXvd5l6sq9A6nv7qnbklImv3o9lPXM8%2b3/3BbLxS8a2h7OQ0NDmXOYPXv2HXGCtEqM59vHNS8WH3ZFH%2bxxvYM7LApx9Dyat8OYOm17stP5bz%2bsPAufv5lbHjpqVumYR/Mq9C4f4Tgf6yaLuhNctGjRnQ6DFlc4yyrxSc8qHXF90l97C2ER65lDy1i1Xx4ybVvyyTPVpurauharPCq7lPF78%2bgpcp52JZ9Ww%2bCoUaNEIuRSwNq1a5UWIAawKxIYyW0lWyjADddCARfqP1EqLpgcGL2Pnl3W6b5ZpYkPzi5jk1267d2A5s8awlOefTUC/dMA5vtzSlmyNaFgd3LG6n2DBkzZ0un%2b7NIuyvkcO3lWo1SABwvwnCWeO3fOa/78%2bVGt7HObWilJOTago5rvAFOPIamx4UIgvDezAOxt6z3Ti/r8kNyUZP46t3wGJmkdmVny4PDpRZGwkFGWB9d0LHvzw4D3jp2OMD%2bwygwaFg6HzygaZPvterYNEC1srcyj5Rz1er0X5UMpKSkh69ati8d%2b8zp9%2bjQlIToBNAG0CVRJAXwVOD2P13Zuzl4KGsJ5oe3Cqzof7GkdQpMOk9ZjUjaEq%2biqc7XMlPM3Huw0buHrg7671kzySE8s2Jn%2b2/wdC%2bn9d0t22R/L35H4QM5rvwOMhoxyDU93yXnTmM8VVsZt2v1%2bR9QIlEIncJl1brKSfLEtFODt7W1E83h8fHzfsrKyAc3NzRGoBil/j%2bb7lyASMISHumgFzsYruF%2bg7QGI4Lm/wFEoG4y2C3lfMR6clA1mPBLboXdh0aFxmWv3j3R%2bezHM6fxP8PMv7n2saN8HiQ2f1sdjYgN//%2bLeYbl/ems4PP%2bSR/Iq5qwuO9KteP%2bx%2b6pr6gZCaff9%2bfiZriiJo5zffBmKNhJKjuGyRipl5bxJhmSPprFy5Uqf/Pz81sJH1M1Wg5ioxzCIhMZ08kyNpd%2bzr96dOP4VVzhEWEvA5AzvvP%2b3uLsmb07i4Y7x7zNpc%2bLD8yrydrx93A6Tt0nROdobkfVH4XzChAmsRQgkJ8j2zu7du8mRyKLq44Sx3NwlUSUqKiwKgybhbd1wNuxJV4j671fNUnWtQ%2bbFilU4wV%2b98FoKzDseji5uxMziEZOX7%2bkJR/jroRnb0wifte6A5jdzKwzY66OfW1E5svLwx92ub4Gr8tvvn5I/a7wg80pVK8xcIad6NZidnS3CYbuFwf9ca4ITnCjDk/dLyy5ihQs9DRfqLd9dbQqg9/GLd3nzUNgfCug9Zk7ZPWMX7Jw3fEbxMND1Rf8QhDrmMN/94LThmWV7HqR3bBfGZ9c7HwmZqHRuexgcN24caxEB2kUBWOEYSmTIw9MkqC9v40E2zsenq0P%2buPNovCIP6Iaw5jN46jZ/TLDzlBV7x2qHFXpj4r1HZpV4ie%2bw%2bgEbd/2FpbRHPjzD%2bMACbk4B4ikpKfFBMsTMA0WRMoNqSx7gygTdM0hyeE7nNz7YwwEPzy3XKXm%2b9uaHoVj5jCFTt3V/KLfcjJWe%2bPC8ch1WPblg67sWKM6UNGGTFduj%2b0Nzyn0VvsP45ecNoVTLYGz56EdVkuOHfMWVB4icxC2jdSmgAx0JGgwGmlTH9PR0I1a%2btWMvm1Kjbjg6M1TNA5C5uYTv9fRGA0LgQ4Oe3zocE/T7w6o3RktSmhZ96cOmFyW4fFPB7i7/vtoUCofYE7jU2LF/9P2hbvlOh6qw1y%2be2zLIgyzqNb%2bbg7wX0MPPz48GfiIuLq5/eXn5oKampmiEwVhu8gJo8kMBcYAY0U/OhsIbWjowTaIkg/qx6nbuNOn3kHpUfGer6%2bxXvvx7FAqVaOT9feDIJs3f9PYUVG85c195K3366n2L6SADocwG8x44ZUXls0u3HRoHJVlQ6VGII2fKxgQkYcUH4XsKbUIWO5ctjstqU86BWzD19W55OB/LHKbP%2bPHjOxQWFjLNvffee1q%2blxhwU6KBvcT5Gsdp%2bSpHiIMNSlL4eZzAxRw%2bdppZB8w8HgmNBWYfmDptW5fnV%2b4d63T%2bj2nw81vD4fT6Itab0HbFVuhPCRm2CDvBxWp7I2nSIDcQW5DK5wie9Wmvy%2bM6NyQZ47jMGgVoPRZDotJ6%2beWXfREJ2P6wWq2eTDlG7HlFn2jDkJgwH5CWVRIAGCy%2bQcYGH%2bDUcFxaWmbJ3XB2Q6VeeV7I3no9tXjXPYQbObPYF/hH75lRZODOUweLYZNEkuQui1GcCYqIonR2JKsH%2bSWPTnDy5MnicNQVBZATtOoEPRQ8YVgJ/%2bsla4U3JhggxkcIDH26YHeve2cW34/8fgRw4Shh/Qj3/VfNZmyBXw6YsjXxkXkVBuASkOQwHlXVdTpspWjuSGWlLPxMMMytxL5hJ8ieVatW3RYFiCggHhGe4K0jsl8%2b0BllbOiY3DKt0urQdoRPCFKGQZg/S3A%2bOHFWL4QVh6LtooC0NJZoSRs2bLilLUBXT%2bxHxCzWXGpqFJcYbM8hD7gbCY4r3Y15bEMAfEHm/iMnWTX32PwdLcZGHUBFV7SSj3ILKBRwa1tAnA0%2b%2beSTt%2bQExdGWu9MhYZD6GhDLfXjdn4gQZ5a6z9OiyElFLcAUUFPr0KKw0fz147Map/OfNI6BvLrieEyjSMRcTlDgFJcnN%2bYEUQ5TcjLabrcPrKiouKUwyKsuF46HrCG8PLWhVI18seTwKKStfVETdP7XlS96IkTeRbFZhDNOS/wSeFUXxfkIfDQPuQM5byXuxsNghw4dyPSMI0aMMC5dujTididCvAz2eCBy5lwtndwGqdD5qN1HEi/iqYJrcyLU4kES1C6pMO1jrLIOe957%2bfZDcta6/fKeQx%2b7jsQUV2Oy2xGcTvgADzjXzZDy4POmnODEiRNZm5%2bf327V4OXmRt97ZxTfi0KHxfiKg8fl23U3qJDh5oqhF154od3L4Xp2KDpWztlwQFxtST8bBSADlPikdeJe4HYfiIjr8X80Ncp0Za2YEN0eWd3ofnQ5ys1aiTMptoBGyHlTByLiWb16tU9eXl67HInRYaQKju79AlRwXmp/IMFDZMjNyPqjIzGdTkeC34dq8K72PBRVHlIKHL83FHeHNjd%2bXTitC0dj88KrB%2bcZpsAJ2hs7FPX19aWjqKh%2b/fqFrF%2b/vl2PxQHudOHi7pD3EZ3gZ%2bK0XnwsQevFedmVdBx8uYxtPxa/kxcjlAO0lgeoXcgSL%2bKpglPf554uRlJTU3/SqzG3ayrX5aiHeK5T1ALuBc%2bPiiFPeUCbrsbEX2/%2bf/0boVsOg2JQuh4X9wKkAFIMwfbt268fbtbV2Y8ePcoKCqTMDLdv3z6mEPq7wurqaqaA4uJiRvvGG28IOltVVZWeh1yG27Nnj8zpggnonfoIR9%2bw8wDQEC2901iEo7HpN/EinvROMhCOZKKHZCRZ6Z1kF/MQ8jgcjpYKiIqKErW5ZsGCBaph8NKlS6oHplevXg0BvccweOXKlXC6v/OE%2b/bbb80Ean%2bwQbQqOAPxvBlZKcp56rciDPZet25dMpRgxnawQmvBAuAcA6HBfitXrrQuXLjQIvqXLVtmnT59ugmr16uoqMiek5MTgGSK4bAi1qysLHNlZWXfTZs2hc%2bbNy%2bI%2bjE24YJnzJhhRv2RgAq0G71TnxiXvt28eXM40dIYNBb109jEA5ZgR7KWBDqTwBGQbCQjrCKFZBayEE/ISuOaDx482MOTAkzdunWzFRYWxiI1thAhERHQO%2bqE4FmzZiUgUQrBb4vAEYBBYHZ2dvycOXMiIUyQOw7jdYXQYaAL4n0Mt3z58kAIZAM%2bhr5T0tG34BcOuq747VIcV14Q8SKeWKwWOJINzjxk5syZiVCAReBEm5GR0Qm0Ua398TQdR3kKL5TJDQVYPODIvB8A2FXGHAYYoIIb2gruLvrjrVbkfIjyOA848kXp/N7DfQ5P8L9Q9/x0xGMymVjMnjZtmpybm0s3RtKkSZNkPz8/fzyMIf2eOnUqcyohISFePj4%2bnY1GI/MB48ePl2EpdMQmrVixgjLNIL1eb%2bQXsTL9ZepLL70kYXUls9ms64AHjonCsAQTlp555hk2rsFg6Ai6QKyYtHHjRonGpLG5nAYkcMH0fwd0vghrkTMzM2V6T05O1pCsFovFm1e7rL9Hjx4ab2/vYOD8fk7/JdKVr%2bad/8%2bR2zShG31oSwX/1ML%2bH2FP9PySBpZdAAAAAElFTkSuQmCC' /%3e%3c/svg%3e" width="450" alt="png" data-srcset="eleijonmarck/assets/static/output_4_0.9377235.f04b0e4.png 450w" data-sizes="(max-width: 450px) 100vw, 450px" data-src="eleijonmarck/assets/static/output_4_0.9377235.f04b0e4.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="eleijonmarck/assets/static/output_4_0.9377235.f04b0e4.png" width="450" alt="png"></noscript></p>
<h1 id="our-goal-is-to-align-a-line-to-this-dataset"><a href="#our-goal-is-to-align-a-line-to-this-dataset" aria-hidden="true"><span class="icon icon-link"></span></a>Our goal is to align a line to this dataset</h1>
<ul>
<li>why would we want to do that?</li>
<li>we can use this to infer properties of the dataset</li>
<li>we can use it to predict future behaviour (extrapolate)</li>
</ul>
<pre class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> linear_model
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> model_selection
model <span class="token operator">=</span> linear_model<span class="token punctuation">.</span>LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>

X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> model_selection<span class="token punctuation">.</span>train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token string">'y'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

<span class="token comment">#For retrieving the slope:</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token triple-quoted-string string">"""
Model intercept (position of the line) \n{:.2f}
Model coefficients (slope of the line) \n{:.2f}
Model score (how close are we to fit a line to the data) \n{:.2f}
"""</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
    model<span class="token punctuation">.</span>intercept_<span class="token punctuation">,</span>
    model<span class="token punctuation">.</span>coef_<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    model<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre>
<pre class="language-text">Model intercept (position of the line) 
6.69
Model coefficients (slope of the line) 
1.35
Model score (how close are we to fit a line to the data) 
0.27</pre>
<pre class="language-python"><span class="token keyword">def</span> <span class="token function">make_line_using</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># y = m * x + b</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> m <span class="token operator">*</span> x <span class="token operator">+</span> b
    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">,</span><span class="token string">'y'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    line <span class="token operator">=</span> alt<span class="token punctuation">.</span>Chart<span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">.</span>mark_line<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span>
        x<span class="token operator">=</span><span class="token string">'x:Q'</span><span class="token punctuation">,</span>
        y<span class="token operator">=</span><span class="token string">'y:Q'</span><span class="token punctuation">,</span>
        tooltip<span class="token operator">=</span><span class="token punctuation">[</span>alt<span class="token punctuation">.</span>Tooltip<span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">,</span> title<span class="token operator">=</span><span class="token string">'b * x + m'</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>interactive<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>scatter <span class="token operator">+</span> <span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">)</span>



m_guess <span class="token operator">=</span> model<span class="token punctuation">.</span>coef_<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
b_guess <span class="token operator">=</span> model<span class="token punctuation">.</span>intercept_
make_line_using<span class="token punctuation">(</span>m_guess<span class="token punctuation">,</span> b_guess<span class="token punctuation">)</span></pre>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 450 364' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-1'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-1)' width='450' height='364' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAA0CAYAAAA62j4JAAAACXBIWXMAAAsSAAALEgHS3X78AAARe0lEQVRo3tVbCVhURxKeCwYYOUZgYBAQhkvAIxM8wCgaDzww8VyN2SUac7jGAxRERUEQAUVRiUlMVBTkPuLBtazHZj2D0ShGzpkIiheecddETdzd2aqme/YxPNR80eC%2b7%2buve%2br1q66urq6/qhsEgpfrEXbu6EIyvtDGxkZ48uRJwYULFwQajYbUhuX7778nhbX5%2bnD7GtKSkpLImL6%2bviKoYqBsgPEToHzo5eUl0ul0gs2bNz8TL773XPm477RQrjZfFCSlHxLsP1b10qy8BxRvWqs6SwhcCWco/eVyuUt9fb0FrL6lVqu1BM2RAlrEYgVtC2xTmgXUVvSdBetHayuD33o69kXezc3NFrDaRrQYY7l06RJ5h32exMPgN5PJykAOIl8jtGvqNKTgGCnZXznsPXTavM0OEIlEDlAHgFn2ys/PH3Dz5k1lQ0ODEzBwhOIADN2gBEDbHooTtPGdPdT9oHhDUSINCvZ3hjIISjfsizwM6fR7pzNnzjhUV1era2pq1FVVVUoO7260rzP91pHyakOnYyqpDP2oTE60L7YDqus0nvduXrHT6e7ZrNy2f8rclJK36zXaXrymABoSx8TEOBnSgZkYmfLQUSFmPPTufPz56ECTY/mNPMxQFkP6uZp61lf4eljW6MAFmWr8cflSk3Objk5OrXNbuXKl6dq1a8leLCkpEQFTIR3AFAqhg1aRLqJ01LQVbQupw0FledC%2bQlr46IyHHRbGG99Rugd%2bQx2ZkI/Okc%2bKLdDNa82i7PJKceuC/mT/0fqSXmMjcydNXlGgZPPVar9v62/s7e0Jo6ioKFSAK7YzMzOJ4FQwE44ChGxgaoKWjM6ZhDttEw/MR%2bf0V2DhKIb1d2eKTtt7XPDtuTo2WXemPI5ScN8TBcRuPyBm81qYWjZt2sqiYPZ71Y6DEvqda0dbQBQbG%2btsSIcPJFCceUwP95%2bMpz/vAHx0oHXFwkN3eVYejY0XZA0arQP77ffhTqtB83aNTco4FPQs20gEGIyTexVwuEd6enrPR48eyWpra62oNzXHFYK6N9Rm1NysaNuHOi4ZBxlwT6vxOw6KtKNT3mbUwbrRNuuPfdRQy3%2b4cdk8v%2bKbrifP1lhx6dTLWzY1Nsqqaxu6X21u8sLJvLdm78D31%2bybCd7eTvfolntVdb0SlGPOeEORodNsi4MiEWoksFevXr45OTn%2bt27dUgIcOlFP64BmB2UgQwFacO/2p0pQcjw%2bemiut3fsgI78kd8rWGhbjwLY93xtg8s/bl1RhqWWTf/8y6Ov6nT3reHdYA46OJ09X99N99MNFUx08LwNJTPmbSgN/tf9FvvH/2yxqWvQDKLKdeAgCbb7PAkFHPlQAAfkodu9KBS423LZ0TNkq2zwgl1DGS1gbsbgLw%2be6m3Y992kvR6zkva%2bZzZ2k7XBdsFJi3nGbLud3dzceFGA46hMoag4zkyPAhiYcJwj1qisNijAR%2bcgiR4FkMbGhADG/a/HqiTTYgsxSMLFEQxbmNXlzHd1PfIqThox2YPCcwaC2U87VVVLJhUPjg5MnvFWIYIZIg%2bbi/5xcHAgg0ZHR5uuWbOm01AAnJkQzF7v7e%2b0NBOBy4%2bcFZymKPDo3nViRWOX5Fq%2bsTRv4tDQLF%2bM8K5dvkgs9Ke714QarR4eVSg7z5gdooA4Li7upUCB2noNb24Q9XnFiJGLsl8Zvig72Pfd7RZI%2b/pMjTmsupKHN/oKCQ%2b9u2EojB/79OnTxzUtLa3n48ePOwUFmhovWIIFWP7lyFkTWGkfcGQW4NwsD39zvissjlSn%2b8EYApuEP68rnqzT/Wim0z00O3bqvAW1RB8aEXLl601lN6c5S4cogCszGmBQnZeXR3KBF4EC3FwA4nFniMgwXicoACtof6flskP%2bX7/xfidhd%2biWoqPvZJZ%2brdb9%2b44tTL7LJwVHBofE704CSJwAv63BzB3r6jXO4CuUVIb%2bVCZ9LkBlxsDJgeUOT0SBGzduSMAR/q4oAIqQwyT0KDBwXobvyPDs8Oit%2bwcJfFdKBs3f5Q/Ob/rQ0MxhPd/drtQ9vOnQQS5gx2Pqz4YC3t6tFgFR4O%2bCAvhMXVmoFAxMMobJK76radAnMhNXFKhGLc4JgFU2X7vrK5NxS/PeAWf3NntfU6/xvNLcJKYHH0w%2bfS7AlQ9lRtmfigJBQUFMASYMBaAWcjw4ooCrIQpQ02YowLSLyZI7XV39GIdOnBOByRP6%2bKh8izeW5Q21n/wJngmY4mRfm5fRB%2bgEjyFrs1j6WcXEN5flzRoTmdsmJG5s9Rci7ph0bztykjImnytDAboIejrvFjh69KhRUlIS0U5GRkYbBTAPzh3gaTB463qz4NPCI1zP7ok1TH7EhOX5xGSnx30ZOjNxT/CYxTmRsNqjofjDpEfOXlccOnF5fs/gpbk9wCpkTDngKzwMkyHq4Jx4YLqNAjgL11YBEokEAwuVWq22TUlJ4cv7jTuI4vCAwtyAJmAxA3t6zNhqMmlFgTUzvaCIbHVQeLaK4nl08JK8CNqeHrwktwggbgxYhd63BIZm2gZF5JAIsL5B68YWhjMmIks3Pp%2bDsvPQXdqcxhobG%2bMk/gQoMKCgoGDw9evXXRsaGjyoB8V95AtlKGZouI%2bxIHPq7RHaXClSoHCeQB%2bOSoAV9/gZAhdITAZBKjoKvPrM0E2lf4TJKeJ3HhwZ%2belfRkCs/0pmWeXYkNW7I6dEF5ZPiSncsbngiP%2bB41Vvbcz5%2b8S9h077QazvpHtwE5HDjfL2pGO507HVVJbuTD4q61Aqu4rOxZ22/Xi3QFZWljQ5OZmY2uHDh8XUnJmXdeOcDolZoMHieLr30fwwaPJMzTtMwtdGaq74nPi2uies8lhwgF4ZpSck4N17wyT7QPYWDZ5/PChBBjG9MVhBPPiA6BkJe1zfXvWlFfLZWXxCgjWdvISOxZyxnAVqXPmoksxoFCji0PUWilGcHiZWr15tAj6A7A87OzsunEiZD2BHzQxmMMjhKvHSxUYhc3ZjInOk3aZ%2bSqwsvfiEGB0YNfV%2buLdHReQsBx%2bwaXpc0cdDwzL7gNOzAx/gDcp4LWHnwXE44dZHxZXFje19Ds2CwbSBfOgDpJx%2bAkMfgO7/VfYjPj5ejwIAh3onCHVHThBx1pLtfXz%2bfvI70fUrF117zdpuCpMJgYkSJ7Rtz3HJvZtX9HsP3r05enHOVFDAJDD75dCeMG5p7lhQQhdymvNxWTA4P%2bKPwGHi3tefFHFgWMA5EXLkc4Io%2b1OdIHtyc3ONfysKVBw9K374wzVnMG8lePt%2bXP7zN5bOAQ8/DCYfDtj%2bUf85O52SMg75tVy5ZN1/TroTbA2yncQjU8SQCFEhPxAA7mOi1E4Bzw0FpFIpbgPz4OBgC7CA7r8FBXS6nwUardaN48EVsLpvT44ukIMCIsDsF4ISFgYu2EVWGnxFrx37TnjAFpgB7/S2fvZ8nQdYxChBQKLEYEzV80QB8hgZGeEqhnh6egYUFhb%2bKhSAQlDgavNFFSQzDAWGQVLjDqvmpvvltlNYatnsabFFeQs2lu4MTS1bFL65PBaSGSXECIEfJu%2bL/TjvMMT3Pzg0NTW6aemYUA%2bHpMgPTN%2bdBj/udPLPHwUA/ggazpo1qwvEAapnRQF62UHMNjS1VJaYfkjy4O41cXWdpgcNdz1g/44Cc/8AApnF4AcCgpfmuUBM0HdKdEFfiAcGFe4/5ZxTfpKYJECfGJyo6PS5WjRfT3CCQoBRIcCfiCPLc0UBdg5A6i1btkgTExOfGQUw/MT0FdsAXSNh33elFw8q64kfI5xFQNkBUZ5fzPb9Yoju5vSdvdMMUOA18AMDsS9YjeL%2b7avWrYcd17hm6nbo63MingTnuaIAed5//31Sx8XF/SoUoKkuUUDUlgqhZ8hWInBWWeUA2OeJ45bkTQpelqOH2eKvvu0/fGGW%2b/jl%2bZbgIAORBmmtAlZYQdtc4bnOjpf%2b3FBg2bJlpP7iiy%2bkvwYFMAPDQwwuLzDv3qGbyt4DBfiB2esdE2yFwO17j/fH9ux1%2byQAcc40ucEDCwX3dolvoh0o5vmgAGUunjFjhgwU8MwogAcNx09XEwVIh28UBoXnDAfTfv3f91scIQCSvR6WaQPbAjzuHBGY/cjkzK8CeXjbAB8bHno7b//CUMDExARz5skeHh4Dn4YCeDeHqwBZmQvAXaDu8W3PtL3HA2YnF8/%2bJP/IKJ3uvgN68OaLja4Yw0NE98fSw2fUGP/fuHppCCYz%2bD3UntRrD4B6ABXW/Qne/sWhAH26DBs2rGtqauoTUeD65YtiSF7IsTRMXgHYPmrM4tzgD5L3kYOHogOnjEE5XobMs8srpUBHz46Xk1yPbM9udqkHb%2bft2YXsC0OBsLAwUldUVBgnJCS0QwH4QMru6iBZafXMvrFGf15XPBN%2bc8x6hgCyN9GPd646By7I7AZmr2BvIJoTMiUaHGbYYuHSOvL2LwwFVq1axS5GeFHgYlOjSV29Ri/8hKh8z9fDst4Ap%2baHR%2bnkBGnX34hQ%2bRXfiMEHOI1YlO0Bnr717E30EVsBdx4hubfDnYMCAH8CegyGCtCjQBONvw%2beOCeDSREnM35Z/ggoI8kW%2bPm2ovJMTVeqJNIX4noRntow3tFb93d0YdLuYqTTUIA9sP8xEOI90QVs7zdkQeY4iOv1%2b7umTuMISmqTC5w9X9/%2b0LGDkyKGAlg6GwXw%2bGkC5AL%2biAI3Wlpcvquu9cK0VvevO45x2w9MWJV2IAZi8z463T2H6roGL%2b0znAgxz8vo1IOrOF69O0WAzkUBSIbQq3fz9/e3RRRIoaHxos3lmNMHz0zcM%2bw/P94geyyj5Gujq81N7XIB7okQVQK77OSlv1QooD%2byqjxllJS4mphHcGSOK8Txk1%2bbt8ueHnGrWvOGx9zDiXYnQvT83Y3He7ehv1QoMGTIkNYTodhok4SktW6jIgv8xy3JHT01uohobP%2bxKogD/hcKc5xMuxMhnlvgp9E7HwUYc916geiDmK1vggLa3J3V1daYaDStt7VlZWXC8vJyIgD%2bLWFdXZ0lPU0izI8dOyaGvu70kFWYnJws4KMXFxcLKQ8FFpIsAQ3fkcQI%2buI32EYefHQ2JsqAspCrdJANZaR9VdXV1aY00xUyularbasAFxcX/fV4UkK8I9%2b1%2bd27d9vdF9y7d8%2bee37Pntu3b/MiCR/9wYMH8ocPH8p/Cw%2bUAWVp91cmIDOLU7hPS0uLMx9vO0ABv88%2b%2b%2bzV2PhEa0iK7CAmUMTGxtqkpaU5lZaW9luyZIkcVsNu3bp1dosWLbLat2%2bfGlbBDbLJrhA4KSCKtN24caM9aNp/9erV1vg9FqRv2LCB0CHossG%2bwEcREREh3717t09RUZHP4sWLkbcC32Ef7AsO2Q6/ZXyQJ6zwABwD6dgXx0YZUBaUCWVDGVHWkpKSfjt27HDCOeD3mzZtsoW2/MCBA735FGDl4%2bOjSklJ8Vi%2bfDlhDkpAgWyioqIcYSBvYEImhXQYyBr6eQLdBdu0ry0IaQf9fWNiYhQcHjgJQmcTQjoIah0ZGekRHR2tQt5IoxO1xb5AV%2bK3tK8t/FaAonriGEiHmEWxfv16a%2bjrAsWLKx%2b2QT5vlB3ngHRQijI8PFy5YsUKlyf98bQP/tWMAa0bPUI3fMZCecWAhoHHFNxZBnR3Sjf07MPx7NSAhrD8ByoL98F7jDcxcTOgoxcfzyPfG3jjzr0IpzL4djh7c3isrKxI9BQSEkKEVSqVRmZmZvKpU6cKt23bJqisrBQEBgYKjY2NlUAnkeCkSZOE9O%2bNJFKp1M7S0pLAz1tvvUXocrlcampqat2jRw/yPwELFy4UYt0FHplMhrfEgvnz5xOaWq0WQXBmC3QzJseePXsEffv2FQHN0tbWlmSjU6ZMIbyBryX0t8Fv4%2bPjBenp6YK5c%2bcKkQbvCEz/8ssvKIMxhDzIV/Yy/ZeIN2elhYL/s%2bd5CIxBkOL3Fvy/7ndS1fzZMnIAAAAASUVORK5CYII=' /%3e%3c/svg%3e" width="450" alt="png" data-srcset="eleijonmarck/assets/static/output_7_0.9377235.fb010ac.png 450w" data-sizes="(max-width: 450px) 100vw, 450px" data-src="eleijonmarck/assets/static/output_7_0.9377235.fb010ac.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="eleijonmarck/assets/static/output_7_0.9377235.fb010ac.png" width="450" alt="png"></noscript></p>
<h1 id="now-lets-try-to-implement-this-ourselves"><a href="#now-lets-try-to-implement-this-ourselves" aria-hidden="true"><span class="icon icon-link"></span></a>Now let's try to implement this ourselves!</h1>
<h3 id="naive-approach-to-guess-until-we-get-a-good-fit"><a href="#naive-approach-to-guess-until-we-get-a-good-fit" aria-hidden="true"><span class="icon icon-link"></span></a>Naive approach to guess until we get a good fit</h3>
<p>guessing the beta parameters linear equation</p>
<p>$ y = m \times \mathbf{x} + b$</p>
<pre class="language-python"><span class="token keyword">def</span> <span class="token function">plot_on_top_of_data</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># y = B_2 * x + B_1</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> b <span class="token operator">*</span> x <span class="token operator">+</span> m
    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matrix<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">,</span><span class="token string">'y'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    line <span class="token operator">=</span> alt<span class="token punctuation">.</span>Chart<span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">.</span>mark_line<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span>
        x<span class="token operator">=</span><span class="token string">'x:Q'</span><span class="token punctuation">,</span>
        y<span class="token operator">=</span><span class="token string">'y:Q'</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>scatter <span class="token operator">+</span> <span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">)</span></pre>
<p>Our guess</p>
<p>$ y = 2 \times \mathbf{x} + 1$</p>
<pre class="language-python">m_guess <span class="token operator">=</span> <span class="token number">2</span>
b_guess <span class="token operator">=</span> <span class="token number">1</span>
plot_on_top_of_data<span class="token punctuation">(</span>m_guess<span class="token punctuation">,</span> b_guess<span class="token punctuation">)</span></pre>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 450 364' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-3'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-3)' width='450' height='364' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAA0CAYAAAA62j4JAAAACXBIWXMAAAsSAAALEgHS3X78AAAS0UlEQVRo3tVbCVhV1fa/AzPKJNzLPFzuZZA0UXg4FEiYM5DmgL0MtbTMzAkHQAERFYcQJUXDnAVRwgGlcrbnnBoiCoLilGl%2balr/9D3rfff91maf%2b869HFKfWf3P963vnLPuPnuvvfZevzVskMn%2bWpf8zx1dzsaXOzs7y48dOya7ePGirLa2lt1N6cKFC4yEZ6k24ramvNmzZ7MxQ0JCFLilgXIw/kzQiMDAQIVer5fl5eU1yFB3QXbruyuy5ZsPyfYeOS377tplWV0TY4pluvCYNkR/lUsHCuZ3zZ8lBK2ENyjM0dHRt7q62g6rb19XV2cPjTGCxogc8GxHz5xnh7sD/81OaMfvDibvBj61pb6vXr1qh9U252RBdOXKFfYbtaG22AH2Vy/X263bcVT9j6/POJ2vrbMHOQr9CXLwdwcTOez4eIa2xOO/2xhrQKHwwS2yVatWIRs2bIi4deuW2/nz573Q2BPkjg/8QR3w7ArywjP95op7OCgY5EY8ELX3Br0E8qC21Icpn3/vderUKfeqqqrQs2fPhlZUVLgJfYPo25cwWR%2b9/pHTpMWfJxZsPhQGJdnXX7zYCWbgQ33ytm5chnAukxcfl547kOw0By4HtafnFyW3AgZQpqWleZny8YGSOpXgk0JsJPg%2bUv1L8cFzJDLl37l5zVP75jLblz9cEyXw2r%2b/KmLL3hOtJPqwIVkk%2bKQIpSkfSjCWw9/fn93T09Ot58yZw2yxrKxMgY/lvCNrkIZ/THyFaAAH/iznQEPK0vG2ck5SfKEPNZHQN/1Gz5fqL2qXbzlk0Se1uLn%2blzu0OLKuSetVxyvOtdTrHyq%2bPPiNTCSfg7BAJJuobw3JLpJPwccxxpusrCx2B0JbZWdn%2b9Ez7nJCTf6BFZ79BOFFA9O2sud8QeukIC1/lgl9NMXHdyoiifZavf5HNs7Q2ZtlOw9VMP63Vy9pTp%2btUYjH5HbuKZqoIJ8fyS70LeYbKcDHp2FHYPsbdkBpaamhI9ytBa3RAMIO4DZlL1IM3ZWiicpFbsmIL%2brboADYt/xcTa3A19acr1NmLN/JJnr5Uj3jV507HwBSPvrxpmEHcCD2Eila4Bt2wJXL9fLTVTWCAjRNYgDMwEvCZpTCACZ8SQxoZGO/wQePUL0RBkARflJ9vJ29ZaCsZZqZKQaQLE%2bKATAvb6M4CF7ADfcXQkND/VevXt32/v37TjU1NWq%2bOs58pcNATiA1JwKvNtxDOPG2LlwpEfw71WP4NPmWRASEWGVVRVWNGm7P6eEP37X56fZ1F3qHMtTXr1520esfND9ZWd0JO8MVu8UF36j42P5cFkdBPrwTP6yu7oI3vlHhW3uYj9OIuVvb4b2dSSAoJxTsrtPp2hQWFobeuXPn%2bSjg4sUI/Mb4JDxNmk%2b%2bJfl3IL/zxi%2bP%2bw7OKh2%2beNNXA1dsPRyMXWmr1/%2bfPdDfd0zujr7YyuH19RfVcIUuvG8jBQigCgU5Vp%2bv%2bxsm7Yk%2brHMK97d9d962d7CDOly5VP/Cn2ICUm7Q1AQ6fbA6%2bNUJ68cm538R6ffGUtuXRq8Ja/POp%2b4Mqwbl0%2bR8n9QNQnEuCzccsOo4anXH6LFrY1z7LWaAWGcqh4eHBwOHqVOn/iEgOG/dPiW5NbpqauvYFhdk6TN1oycUAFf3iMxS1nvKhpDYKRviRJPVkl1fu3JJjOoGEMRKG/Bh5qo9rwzMKBnUfWKh1kTpxvgya9asP8QNErILY8Ymb4gKeqvAGT7d7uc711sQb3XZYflrKcX2UECr2zevGq30yPnbBN%2buFfl59htMwg5KNADb69M22XcZv77blPwv3kASxVZ9QHqJAtgi7QZjY2OFQMiggMWLFxspQNCaENzw50YK4MGMlq%2buTFjp7QdOKfGu45N/8bXUYibwjgOnvL45U63uMmH9CPBfHphZwlB79ILt/dEmqOfkomCT1dMKO/D2jauMd%2bybs476X%2b8wVxqfUhwRl7zhteAhBU56/c/quzevWTaYwy%2b09aUVYHA9586ZzZgxQ8rGLJpwYZQnNDfhNY60/uvamGK6JhWGvTJuHRM4LqV4Tp%2bpxUO6Tlgf3WNSUVSvyUXxIUML7JH8BODZDeFvC%2bCCU7ekwnBqD2DzN01noWRFXvFXYS%2bPXtMD2z1U4FeePe8Hj2EhIbfxHK2trclufCIiIlyXLFkSiA7NKisrKfy14pOnUDMIdzMKLDgmEDBqORITn3aJJWVaoBD6DgGLtV7/b/NxC3eoPsgp08D/BtF4w%2bduDQMaB9LzmzNK30uYXjJy467j5gPSN3WMSy6egnsnyOAu7J7MFbtdxi7coSGQhlsLwcRsDxyrZAHOtE92Wr43b1vf8Xnlb09e8jkLy785U2Oz61CFOeQI4mGyBZeP5DYD6YwUYG5ubkeyBAYGhm/cuLHTzZs3vZENamjb08rzjiI56Gk4UYbXEffWIG/O8%2bMuqTN9hy2v%2bee9G14ApoislbujVm07PAwTGYCJ2M1YubtL0sflr%2br/dbtl0efHumESPd6aWZrdP23TTCB3OGy330fr98eW7v66za8/3fRAP55wlb5Y0Vfu3foWWeIPrrlF%2b3tDkUPzS/7RG3FDO/DdoHQtl4WCoEi%2bcDQHPx4a%2b%2bDeVtIEcnNzLTIzM6WyPrOnyQYbbTF%2bfV1xTgd31BXujeFGUOInfqvLjngh3A3zHZTvSmYRNuJTs5hx6xInflw%2bGu%2b%2b8i7zrY23%2bwNV2Lsrm0WPXfcqqKNgApVna9ybiATNHuumhw8fzu5wgQSCzH63bt2qkHKDHOTEbtAoGxRMQ9z/sNlb5AhwzOo5H3YfA3cXiInGYwt3/2BB2fsdRq0O5CDmBSxIxOoaXN/4ReWGcDZ9%2bc7XAY594lM2GFznqcpqR3EuIM76SHbTDLQRRiUnJwvu8JndYG1tnQJ2quU%2bPLzXlCImwKETVQpEer4iN%2biG9Lbb3zM/S4SvngZg7In2sSAP%2bj0pr7xHn9SNPqL2HgC4uIzlu%2bIFHsJaxe%2bSDSYmJrI7PMAzK2Dr3pNK2KN31Ji1rpjMAMaMmS9fX35UWfzl8XayqDlKrLIdJqSVaVOVCG/bvzd/W5fID9do4L48BZnWbj/C4nXd4GVyeINo9NWj9dvLAao/upftP6mUNRuDrf9Q2OrPpgDhKioqskAwxLYHkiLJOMAklTWKBLmdK298e1kT/t5K675TNxpleYjFJ/aYVBiGyURjsjFwcf4rth6iqM8eIe%2bL/dI2qkS27gZADMHkX2fK4ldV9fmAer6V6yTSYfECcVC2EgVljRTQjEqCNjY21GHzXr162WHlpcpWFk2Us6h219wYpO5SOdtfeMdOUGPrjo1PLfZdANTGRDV4T2g7YgXzz3PW7A1dvuWQLvLDtfGIANkqth2xsjkCoWGYfJJ7/yWWPL4TFkNTXVNrKkdzkkUq9yDZJfgGU%2bwGam1ra0uDvBUQENBh06ZNL9%2b4ccMPblDHw04CkhAQuTZf8qFEvHMqcoYSH5MmHgUpAaAYCliQwmr%2bdf%2bG96xVe2JhrynpBTuzx%2bRuH48QdTK2ryuyvY6jcsrS4fbigeRW%2bp%2b/95qxYnfCO3O2JiJCHPTLjzd9kCJrEEDpeARIBY4YUACNxfMCPy4DyeIjyMdl7cxl1/C5aPlzO%2bPivI7FBRbDhg1r9tFHHzETOHDggJIjvpBt%2bYsKpEq%2bGt7iTI4CF6yOGfw/Q3SAmx/I8/VpG937pBYnbNlzoh1WNRjvLyZklGgAgtG7Dld4nDl3XtV5zFo3tOkPswmjb%2bH3ffccPm1%2b4vQ5OdJXBUd3GZ%2b8mTgp42m1t6l8XEk23DspRHyNaSjJ7vn5%2bZbwBMw%2b1Gq12G9ainIBceLjCZtkGICJRcHHM5cIJeg4rzNi%2bb/DrXWC63IBBrxJfPDC4B260vNXx8%2bosd37dh67tgts3VYYEwGPFrtKLuHD/QW8EfHoDMDTVD6OAZbiMF0SBEeOHCkURw1eADGBXJTJSYIgKYAOL7j/dkLezYIOrFpQt4mF8QC7REy0Zbt3VzLNz1%2b3Lw7uLhwTdYdCnAkPEMlRCBtuiEnmGFybVpR2iyfVKBvk6bCnFAiS7L8FguxatGjR/6QAcTYoXD/dvq4gm0WQ44dJGgEkQlU/QVlQznQAXwfE9a7I1CgUl60pOyLX//uuzDTtfe4K6NmzJ7svW7bsqU2AjpvoGavJeIs3faVEnO6OMFWHrd5aFMj0mb16TzTSXnJtffDOagDXrlxyPn22Rs0zvd/c6s/NBITa4JAhQ54aBIVTHaEcDZMwq6%2b/qOufvskM6G4OwGNKALh1RsKTgtU3OpaqazjCchX13QjshMOY5waCSIdpm/TVarUvlZSUPLEb5K4nlLsiLXeDOoTCXc4i90eAo567dl8CQO6dKUu%2bGHSysronIjlXZHU6gFwAz8wiOPnwPppyd8/PDTZr1oy2lF337t3t5s6d6/0sgdDQWZsVP3x/zaeh8LE%2bAGDYPWb8OhZ4fH/9ilSxxZlIgq8RbPxx/GcJhIwuBEFPFQrzdJOBYPaavYrgIQWsfdm%2bk20Q8sbBzmOGZW9m227ltsNmFCzRM3aA%2bFRHfDQmbwrspPgiMP6fQ%2bGGrGrEiIbKS2bmUyVDBDzI/oy8AGy8w7hFOxJ7TCzsBaRnuylkaIHiu2uXn/psUGTjkvzf7WwwNTX1idNhDj6sI0Rw3khxmQnEJRd7APXjKZJDNshMYND0EjPRtvvrKgARoIxP2ko4FzAtiAjIiZWUlx84pRQOHvYdrXRBEhMZl1LcE36/Ga8G64QI88Tpajmv4SuFWpxJLMFOmiR2l45/Y6R0Uz7nOYhM4OkLIsKVl5dnMX36dMmSmNTJ0KTFn3cckL4pITDxE68nKYk1carjRPSkQPU0J0NPXBKzsrKiul5vZIN/ExdFuYthRVEAV2eAmDfV2rfsPdFmyKzNA3MK909A3N8ark2FnIBid0NRlH8nFCL9eADTWShQCoVLfuYYJiq4CoXYzvwbcR9GfN6eErLWvEDrKSraevFC7uOLopaWlua4%2bbZv39516dKlhrJ4PeznbHWtxfGKc06P7t%2bgsjS5ucg3Mj9LgGtT6fU/uV29ckm1ff8pC6SthBOWPPAI4eV0K05G5XLOE0rrwt8FKTlPKMWH8G8shX5M%2bTxEN%2bM4ohX64KV7M14RfnxZ/HEHI3Qt/exgh6gxa3t2n1jYRuBVVNV4YdWf6GCkKX5TcQBfRdmT8Olwhg5ppLa6VBzQyESjo6MbHY29n7WKAZ1DXJ4SOX23D3O3J3oOWMIQdVXZEQFMPIU4QOpozATtjfgiRGZH5eIDVqk4QIov9gLiZEgEsE/mBTDphrJ49iyrrNlzDOUs%2bPEgrPhrI%2bZubaXX32cgk5z/hfLbq5fkomTouf2N0B/mBuvr6xtQPXW6Tf78DI%2bI8dtsYqcUxfaeUhQluLPKqmom/O4vy%2bU7ysuZAPS3hNXV1fa8oMo6P3jwoLK2tuEMcN26dXKE1jIp/rZt2%2bS8DxURPROPfmsor9dq6Rt6pj6k%2bMKYJAPJQs/lkG3Hjh1CW01VVZU1d/VygV9XV2esAF/f/74Pn7Y0OmZiSfeopFKDa/pVrze/e/eup6kt3bt3D7m8vtHJ0O3btyX/QEKK/%2bDBA8eHDx86PksfJAPJYsqHzF50nmjKJy8n1bda5%2b8XOiPnk5jMGTOd1uZMsc%2bcPc81I2O684pPP/Xavn17%2bOTJkx2xGup58%2bapx48f74BgKRSr4J%2bcnOyEAEo1c%2bZMlwULFrhC0%2b2zsrJawLRURMTPyclhfITbztQW/aiSkpIcS0tLWyIDbTlx4kTqW0W/URtqu3DhQjV9K/RDfWKFI2gM4lNbGptkIFlIJpKNZCRZy8rKwlesWOGVkZHhTN/n5ua64Nlx165draUU4BAS8oLm49y5mqmpKS6zsuepkRiRQM4pKSmeGCgYnbBJER8DtUAIHQC%2bLz3zti4QUo32IWlpaWwyAh/fMb4wIeJD0BaTJk3STZs2TUN9E49P1IXagu9G3/K2LnhXQVEv0BjER%2biumj9/fgu09QUFiuWjZ8gXTLLTHIgPpbhNmDDBberUqb6/9cfTLemvZkx49N5Voi2VktqY8Mjt9CPLMuFrOd%2b0yhNDf6dswqO4pD%2bXRXxR0TSOn2eIL8KqeAn56C8/Ooreg7kMIU3OvjkuBwcH5jsHDx7MhHVzczO3sbFxHDBggLygoEB29OhRWWRkpNzCwsINfBYH9O3bl7V1d3c3Q2Cltre3Z6WohIQExnd0dLS0trZuERQUxP4nYNy4cXK6N8Nla2tLfzkuGz16NOOFhoYqEJ26gG8jyLF582ZZWFiYAjx7FxcXUpCsX79%2bcl7QsUd7Z/oWcYxs1apVslGjRsmJh99Yye7Ro0ckg4W5uTn1a/tX%2bi%2bRYNFKy2X/z67fQ2AXkOqPFvw/lus17KYJkvYAAAAASUVORK5CYII=' /%3e%3c/svg%3e" width="450" alt="png" data-srcset="eleijonmarck/assets/static/output_11_0.9377235.4a663e8.png 450w" data-sizes="(max-width: 450px) 100vw, 450px" data-src="eleijonmarck/assets/static/output_11_0.9377235.4a663e8.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="eleijonmarck/assets/static/output_11_0.9377235.4a663e8.png" width="450" alt="png"></noscript></p>
<h4 id="not-the-most-sufficient-algorithm-but-might-work"><a href="#not-the-most-sufficient-algorithm-but-might-work" aria-hidden="true"><span class="icon icon-link"></span></a>Not the most sufficient algorithm but might work.</h4>
<p>hmmmmmm ¯\<em>(ツ)</em>/¯
let's think of another approach.</p>
<blockquote>
<p>Can we we somehow see if we have a good guess?</p>
</blockquote>
<h1 id="lets-improve-our-guessing-strategy-using-gradient-descent"><a href="#lets-improve-our-guessing-strategy-using-gradient-descent" aria-hidden="true"><span class="icon icon-link"></span></a>Let's improve our guessing strategy using Gradient Descent</h1>
<p><strong>Gradient descent</strong> is an optimization algorithm used to <strong>minimize some function (loss function)</strong> by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.</p>
<p><img src="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent.png" alt="landscape"></p>
<p>Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient.
We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.</p>
<p><img src="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png" alt="winner"></p>
<p><a href="https://math.stackexchange.com/a/1695446/196117" target="_blank" rel="nofollow noopener noreferrer">math.stackexchange - Partial derivative in gradient descent</a></p>
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" target="_blank" rel="nofollow noopener noreferrer">ml-cheatsheet</a></p>
<h2 id="lets-introduce-the-loss-function-or-costerror"><a href="#lets-introduce-the-loss-function-or-costerror" aria-hidden="true"><span class="icon icon-link"></span></a>Let's introduce the loss function (or cost/error)</h2>
<p>A Loss Functions tells us “how good” our model is at making predictions for a given set of parameters. The loss function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.</p>
<p>${\displaystyle \operatorname {MSE} ={\frac {1}{n}}\sum <em>{i=1}^{n}(Y</em>{i}-{\hat {Y_{i}}})^{2}.}$</p>
<p>Given ${\displaystyle n}$ predictions generated to ${\hat{Y}}$, and ${\displaystyle Y}$ is the vector of observed values of the variable being predicted.</p>
<hr>
<p>Our example with
$ \hat{Y} = mx_i + b$</p>
<p>Now let’s run gradient descent using our new loss function. There are two parameters in our lost function we can control: m (weight) and b (bias). </p>
<p>Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the loss function with respect to each parameter and store the results in a gradient.</p>
<p>Given the loss function:</p>
<p>$ f(m,b) =  \frac{1}{N} \sum_{i=1}^{n} (y_i - (mx_i + b))^2 $</p>
<p>The gradient can be calculated as:</p>
<p>$ \begin{split}f'(m,b) =
\begin{bmatrix}
\frac{df}{dm}\
\frac{df}{db}\</p>
<pre class="language-text">\end{bmatrix}</pre>
<p>=
\begin{bmatrix}
\frac{1}{N} \sum -2x_i(y_i - (mx_i + b)) \
\frac{1}{N} \sum -2(y_i - (mx_i + b)) \
\end{bmatrix}\end{split}
$</p>
<pre class="language-python"><span class="token keyword">def</span> <span class="token function">step_gradient</span><span class="token punctuation">(</span>m<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> b<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> points<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> learning_rate<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">list</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    this calculates the gradient step of a **linear function**
    WILL NOT WORK for multiple dimensional data,
    since the derivates will be on matricies instead
    """</span>
    b_gradient <span class="token operator">=</span> <span class="token number">0</span>
    m_gradient <span class="token operator">=</span> <span class="token number">0</span>
    N <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> points<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        y <span class="token operator">=</span> points<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
        b_gradient <span class="token operator">+=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">/</span>N<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>m <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
        m_gradient <span class="token operator">+=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">/</span>N<span class="token punctuation">)</span> <span class="token operator">*</span> x <span class="token operator">*</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>m <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> b <span class="token operator">-</span> <span class="token punctuation">(</span>learning_rate <span class="token operator">*</span> b_gradient<span class="token punctuation">)</span>
    m <span class="token operator">=</span> m <span class="token operator">-</span> <span class="token punctuation">(</span>learning_rate <span class="token operator">*</span> m_gradient<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>b<span class="token punctuation">,</span> m<span class="token punctuation">]</span></pre>
<pre class="language-python">new_b<span class="token punctuation">,</span> new_m <span class="token operator">=</span> step_gradient<span class="token punctuation">(</span>m_guess<span class="token punctuation">,</span> b_guess<span class="token punctuation">,</span> data<span class="token punctuation">.</span>values<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span>
new_b<span class="token punctuation">,</span> new_m</pre>
<pre class="language-text">(6.6874785109966535, 1.3465666635682905)</pre>
<pre class="language-python"><span class="token comment"># Let's run through it through the whole dataset</span></pre>
<pre class="language-python"><span class="token keyword">def</span> <span class="token function">gradient_descent_runner</span><span class="token punctuation">(</span>points<span class="token punctuation">,</span> starting_m<span class="token punctuation">,</span> starting_b<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>
    m <span class="token operator">=</span> starting_m
    b <span class="token operator">=</span> starting_b
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>
        m<span class="token punctuation">,</span> b <span class="token operator">=</span> step_gradient<span class="token punctuation">(</span>m<span class="token punctuation">,</span> b<span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>m<span class="token punctuation">,</span> b<span class="token punctuation">]</span></pre>
<pre class="language-python">number_iterations <span class="token operator">=</span> <span class="token number">10000</span>
<span class="token punctuation">[</span>m<span class="token punctuation">,</span> b<span class="token punctuation">]</span> <span class="token operator">=</span> gradient_descent_runner<span class="token punctuation">(</span>
    data<span class="token punctuation">,</span> 
    m_guess<span class="token punctuation">,</span> 
    b_guess<span class="token punctuation">,</span> 
    learning_rate<span class="token punctuation">,</span> 
    number_iterations
<span class="token punctuation">)</span></pre>
<pre class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Starting gradient descent at guess_m = {0}, guess_b = {1}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
    m_guess<span class="token punctuation">,</span> 
    b_guess
<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Last gradient descent at guess_m = {0}, guess_b = {1}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
    m<span class="token punctuation">,</span> 
    b
<span class="token punctuation">)</span><span class="token punctuation">)</span></pre>
<pre class="language-text">Starting gradient descent at guess_m = 1.3450919020620442, guess_b = 6.687439682550092
Last gradient descent at guess_m = 1.4510680203998683, guess_b = 1.4510195909326549</pre>
<pre class="language-python"><span class="token keyword">def</span> <span class="token function">compute_error_for_line</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> b<span class="token punctuation">,</span> points<span class="token punctuation">)</span><span class="token punctuation">:</span>
    total_error <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token comment"># sum (y_i - y_hat_i) ^ 2</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> points<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        y <span class="token operator">=</span> points<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
        total_error <span class="token operator">+=</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> <span class="token punctuation">(</span>m <span class="token operator">*</span> x <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>
    <span class="token comment"># 1 / n</span>
    mse <span class="token operator">=</span> total_error <span class="token operator">/</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> mse</pre>
<pre class="language-python"><span class="token comment"># lets see how bad our guess was</span>

compute_error_for_line<span class="token punctuation">(</span>m_guess<span class="token punctuation">,</span> b_guess<span class="token punctuation">,</span> data<span class="token punctuation">.</span>values<span class="token punctuation">)</span></pre>
<pre class="language-text">838.9099083602013</pre>
<pre class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Starting gradient descent at \n guess_m = {0}, guess_b = {1}, error {2}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
    m_guess<span class="token punctuation">,</span> 
    b_guess<span class="token punctuation">,</span>
    compute_error_for_line<span class="token punctuation">(</span>m_guess<span class="token punctuation">,</span> b_guess<span class="token punctuation">,</span> data<span class="token punctuation">.</span>values<span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Last gradient descent at \n guess_m = {0}, guess_b = {1}, error {2}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
    m<span class="token punctuation">,</span> 
    b<span class="token punctuation">,</span>
    compute_error_for_line<span class="token punctuation">(</span>m<span class="token punctuation">,</span> b<span class="token punctuation">,</span> data<span class="token punctuation">.</span>values<span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">)</span></pre>
<pre class="language-text">Starting gradient descent at 
 guess_m = 2, guess_b = 1, error 838.9099083602013
Last gradient descent at 
 guess_m = 1.4510680203998683, guess_b = 1.4510195909326549, error 111.87217648730648</pre>
<h1 id="how-does-gradient-descent-work"><a href="#how-does-gradient-descent-work" aria-hidden="true"><span class="icon icon-link"></span></a>How does gradient descent work?</h1>
<hr>
<p>In it's most general form:</p>
<p>Gradient descent is based on the observation that if the multi-variable function ${\displaystyle F(\mathbf {x} )}$ is defined and differentiable in a neighborhood of a point ${\displaystyle \mathbf {a} }$ , then ${\displaystyle F(\mathbf {x} )}$ decreases fastest if one goes from ${\displaystyle \mathbf {a} }$  in the direction of the negative gradient of ${\displaystyle F}$ at ${\displaystyle,-\nabla F(\mathbf {a} )}$. It follows that, if</p>
<p>${\displaystyle \mathbf {a} <em>{n+1}=\mathbf {a} </em>{n}-\gamma \nabla F(\mathbf {a} _{n})}$</p>
<p>for ${\displaystyle \gamma \in \mathbb {R} <em>{+}}$ small enough, then ${\displaystyle F(\mathbf {a</em>{n}} )\geq F(\mathbf {a_{n+1}} )}$. In other words, the term ${\displaystyle \gamma \nabla F(\mathbf {a} )}$ is subtracted from ${\displaystyle \mathbf {a} }$  because we want to move against the gradient, toward the minimum.</p>
<p>Here we have defined and the algorith works with contraints:</p>
<p><strong>learning rate</strong> ${\gamma}$, for small ${\displaystyle \gamma \in \mathbb {R} _{+}}$</p>
<p><strong>function</strong> $F(\mathbf{x})$,  if differentiable; then ${\displaystyle F(\mathbf {a<em>{n}} )\geq F(\mathbf {a</em>{n+1}} )}$</p>
<p><strong>Leading to</strong> ($\leadsto $) a monotonic sequence $F(\mathbf {x} <em>{0})\geq F(\mathbf {x} </em>{1})\geq F(\mathbf {x} _{2})\geq \cdots,$</p>
<h1 id="coming-back-to-the-real-world"><a href="#coming-back-to-the-real-world" aria-hidden="true"><span class="icon icon-link"></span></a>Coming back to the real world</h1>
<p>Scikit learn provides you two approaches to linear regression:</p>
<p>If you can decompose your loss function into additive terms, then stochastic approach is known to behave better (thus SGD) and if you can spare enough memory - OLS method is faster and easier (thus first solution).</p>
<p>1) <code class="language-text">LinearRegression</code> object uses <strong>Ordinary Least Squares</strong> solver from scipy, as LR is one of two classifiers which have closed form solution. Despite the ML course - you can actually learn this model by just inverting and multiplicating some matrices.</p>
<p>2) <code class="language-text">SGDRegressor</code> which is an implementation of <strong>stochastic gradient descent</strong>, very generic one where you can choose your penalty terms. To obtain linear regression you choose loss to be L2 and penalty also to none (linear regression) or L2 (Ridge regression)</p>
<p>The "gradient descent" is a major part of most learning algorithm. What you will see most often is a improved version of the algorithm <strong>Stochastic Gradient Descent</strong>.</p>
<p>source - <a href="https://stackoverflow.com/questions/34469237/linear-regression-and-gradient-descent-in-scikit-learn-pandas" target="_blank" rel="nofollow noopener noreferrer">linear regression scitkit</a></p>
</div><div class="post__footer"><div class="post-tags"></div></div></div><div class="post-comments"></div><div class="author post-author"><img alt="Author image" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 180 180' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-15'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='5'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-15)' width='180' height='180' xlink:href='data:image/gif%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAACXBIWXMAAAPoAAAD6AG1e1JrAAAOUklEQVR42u1aCVQURxoeZxg5B4FVOSMaARWMSDRBQRRJAoRDRCIe%2bNQYEDYIHiio65HoegGKKB5cRo1o0KwSQTCKDCLBDSQoHogKyCkoIARBYHq6/q1qpoeGsNm3%2b96S2Sz/o%2biq6uqu%2br//rJrm8QZpkAZpkAZpkAbpd6AhsvL/QUrCofK6imiYgHOL/8cXNb%2bHx4WbIlip6%2bMykgvC8KCiPzYOAMDW3fVt5reo6Y2twvUJpEOorKw0zchUkOO1jW%2bhpc%2bAdMNtw/%2bO2XCYY6SpqanZ7zidEbon7RYFwDu%2bEaD93nzgiwxa1YU8u77jomYvZxjGRMxF8BvzKRTxuVe%2bTPV9dx5mKmrDtGPm%2bgaDd8gO6Zg5a%2biRThukylP8gSfQa8K352mrqK8er21wTUtF/ZN%2b3k2ciDEupgrFsaenJ6OShoaGRywsLEpx1VJ2i5GaqroGK72PPljwGYQdTaLnhUbQuo7BoDTOHUSmM6QOc2bB/jAfKAo5AhVLDoCZmk6h7Bk9XBbgchqXh06OjlJbW9sWXH%2bLmUAg%2bH3NAasnK/FpeGFQXVUN%2bnr6rbg9SdYvvH8fmDE6Bm8VuAdtA2uPZZS5oze8O38J2hy9DuXf3wQd9ZE0nDknhV0ZkrIFe5Amj9%2bMH0nT1tJ65ezsDHGxcXD9%2bnVoam6GE4knfsH3RpB3amho/O7%2bgJVu3ObNm7FZQvvPP/wdtDREFbhPmzNuheHYCeCydK3Ue8sutDX7W3S26SeoQpdQG8Si0rRIaAw5jSTrklGO8wako60NycnJUFFRAXX19ZQ4J1uyZdu2zrz0G7DI85NcRfEBDPpCoVAdX6pv374NBQUFNF5YV8bZv4ESj59AQr67/cJdob57X106lAcX85roqMpyOPLyPjrzPAWa6JOQcGYt3PjLRoAv0qBrQzJaa2wD%2bmNGIfweCheUdjEFju49gO5cyoRdAesJ14x/EKmoCTw8PBTC4TmYmJgQ6dOBgYEoYn8kWTwsmffJm8%2b8NtRFrD8Fn81dB/mXX9PJJa2wPesaWnf1NNyDs5CZvRscP5oK8G0K5PpFwVITa5igpQtTbaaj103N8PWxBCo0IBgtdvUEM6PRL/FcX5AJF0/9kCczAYVQfx9LS0vCs/TKlSsoKCgISkpK0Oql22BX2GGwencStd3/GJ11txPOPWxCy77cg840fg8FZZFgMt4QDm1ZBDErV4OWuiaYCkSweOEi8PNfiaBDCraWUyUyJ/gOLn9S1JBngb0xlJaWQllZGTp2/Cia5%2bYN/p8GIde5jvRW/0goKsSqXCVBQdsjYL34AsT8dAYmWxgTdYawDQEw0%2bEDwHYEq3z9IS4hHmxnzGBM4EJMIhmTxE6oJdIUKCoIMe7u7kQLurKzs9GOHTsgLGwjOrQpCT0qANibnAnTnD1hxmJP8PhyHYRs3Axb/rIVbtzIguTz38Lot0aBppo62OBIYjN9OgNM4d07CBrb6Snmk0jbq4/WKQZhB8htZoSHh0N5eZnU3XUOWuj4Z9i1%2bjiym%2bUCS5YtBaGAD24ecyAz/Sp2bYybgK6uLvD39wfiQH2WLGEYx%2b8kV5SSepkxq8j120j7RzKBs52DQmd/Snjx34wePRoWL/Kh79y5g6JjokBJiQ/YWzPMLV%2b2nGGcoiiMAYL29nZgKTGRUXemiEQiqKyqJCihUnEBras9nPQ7KPLukbuo3REREYz0iouLEZ8vkDNmZ2fHMEuYp2maqZOrVCpl6gcOHACcWQI2I2YYhfCYlk5qqasXef6kLPkS8BSU2A3/mt27dxMGJB0dHcjKykoOQFxcHMMoK3kCBJKZQ319PXCJAEMTW8FYnQs/Sp6vMtTV0%2bTmIIpGrGQio6KiGADIv2fPnkFoaCgkJSXB8ePHISQkBF6/fi0HgJV%2bfHw8vP/%2b%2b1BXV9cDQDc46F5qNsIRgIBgpbBmwFHNI5gYU8dODnGlSpwkzhZ7GJQVVguWL18Ot27dYuoEGFk/anlQIbU0nUAA8FfIaCDb%2brKLiiGSJhqAnR0iDBKnJ5FIeqk3yzRL3DZbRzSxAPxX/5pysbEnAOxnHa4im8Dhffv2sQD0snMi1W6pd7cbGhogIyMDKisr%2b0pd/hzjB7pAsmLeIgJAvCIDwC5qo89iHwYAzBDq5qU3U6zd%2b/n5Mc5R5jTl4bHX2O4OyYr5PmRsnCIDwDomawMDA2htbWWYxxJHXC3ghkAXFxcGgJSUlN8AgFEXyt3BkYzdq8gAcE9o0mS5AHGEvRjigvHq1St48uTJr22/zzhpeydlbmJGAPBRWCfYRwsmqqiodBUVFTH8EBC4zPXn9Poyz6bKxGeK8Z4Bv7N8uO5Ihc4DmCNc5nx/%2bHByGlRjamoK%2bfn5NMsoUXE2ArCm0K/jw/2s5jQ2NVFm48YRAFYpuvS5khmpoaHx/MKFC%2bDt7U1H7t8PHZ0dvwp7UgwIAYVc2QjBpUdPH8PK5Z9RC5zmEACCmHRTKPyfAEBXS0urobm5mVHhQ%2bH7ITggEGKPx0LBTwXwqqUZ/hm9wUCVPH3CZIbrfQPhaVoudTHhawJADHmxirKK4gLAOSV%2bj%2bwKW5qbiUgZ/W5vaYWb6dfhePhBCN%2b%2bE6J2h0Ps4SNwIi4eTsTGw%2bn4E3A28RQcizgIBzbtgNTor%2bB51h2AX7qkubm5BIA88mJzc3NFVoCe/cDcuXNlZk8hShb35er/pgtqi0uh%2bGY%2b3E2/CUWpYniQehNK0n%2bAxntlQFU1YsQkOIQw2SOqrqkBTU1NRJxrH6AVLwSOHTtWB19eymI7TQDo5fRoKfwrkquNLDskQLq6ugJ7KKromeDmKVOmgOxYm0kD%2b4vvTJ5Py%2boyhlE/%2bQIBkGBy7tw5AkCpr6%2bvsgxwxYv/xsbGuvjSkpqaKpc%2bNxWGPglOX2ZR//eZdxBbMjNjkqHFihYOuUnJxRUrVjDSZ7ey/SU%2b/6rNBYGYTWFhIWlIExISCAA/sPPq6OgoFPOxbm5uctWPPhiN8H7gVwz1zQY7Ojr6zQ7ZDZNYLAYbGxvWQmjZCZOTImgBl/njS5ctY9ZNy/a6jo6OiN3kSPtGAc72%2bOH9B/L7XCDY84Pg4GDAEUXuEwkgeD7m98GWll%2bGKALzh2fPmgnlT0oojvNGW7ZsAZlGyNPdvpImGWDmd1eAu2Hqu2O0t7eHfV/%2blelua2tjTOFgdDQBIZRMrjpUqCQUDbApsL/MqqsoH1n550DYc/o7yi3oC2Q9YxZ8n57KcHH16lXAGyJ4Uf%2bi1ykQFwiqUwJfRR%2bFttbX/e4CJRig8ePGQ9bFK3Lt6ezsRBXlpfDxhw5kEXNYU9AYaTQwzIs05T9PhcXEfwVXG0ASnPUSrbr5C4wK/QbxlDUhPe076Gh7AwLeEEhITOx3r88w09IGUVt3QXNDY69%2bVvpF94pgkvlEqL/3lGl/feYM%2bhQ72UOnz9OfR52BYQajpXgdvqxWClQ1By7vcZhl9zCrqh0ck55KP00pBd%2bUp2h%2bRhOMCTwFWsP14FXNc5g%2beSq4df9k1ssMkOxIrL2%2bCTb6roKGmrqeM8Du2M%2b0iaovcPPEDwOE79wNNrY2kPu0Hp0sboeQWy10QHIhqI8wBA4IggGJ95hMFnrP77xQLoGPk0pQYEYlWnSxHPwul0LozVbgTfRCO9aHQMKBI0hFTRW9aX%2bDuEkRC0BLzQvwdnKHlxU1MgAYM0GyozTw9JqHvj4Ui54Xl4HxqLdQwaNy%2bLasC%2b3JqUabrj2Dgw8k1PsB%2bwgAtfrj3hmQswIWAFM72%2bmS6IJGcD33BDkmPUGzTj2Gtd9Xoi05DchwWRxMnmiF6u88lmqLhqHzF84j1n6JJrASfvmsBhysbaHuaYXcTCiqm/nmlhY0w8YWvS6tla5Z%2bTkKWrMGMl8A%2blJcCX5pFbBVXIMul7ai%2bcfyaZ6yDjY3ns2A/GbAOkAhj5d/%2bFIWrLjeRHmcfQROGISNmVXIL70WRF7HpFaWU/EmJwvG6BmBk7MzcCME69Ban78AKzNzaK2o7RUlyb%2b8vB/Q2NFjoPbHh/CJk4s08cJ36ODdNtgmrkYbb1RDYuFLdLu2DbxPllACA8sBzQ3YCVytp02DsBu1tP35WonPpWfSvXn10pXiNxLR7A2gq60JZmbjD%2bJxLqqqqtJ9x07A3y5fQWlXrqDrmZlInJMLMYcT0dsmE9CldDHczM1D4uxslCUWo9uF96SfB68lTK13mv3RKasJEyD8xDd0cgWiDuVVo1XpFWhTZiX6a04tPXnnj7SyrinN7hQH6lcjdpIvzKfZg1f8LVjzfRVsy6oG%2b01fgVBzxCNej0oyucI7XgGQ%2bHNdx7WfSyhsy1TG4wbKdn2M1MDMAiUW1lM3HlRSN%2b%2bXUuJ7pdSB3Cqp0aRp5OsQNrbtmWI7C3bcqoM1OS3gfrEW%2bWU2SgOuNYPJihjARp9BBulYzByYxEhVTZ3b/HioqtppE%2bvZBUbm797B7WhcGIc0fbIl%2bwGBqv7b4x6fzbkHf39cDXnFz%2bBycQNYLQgi39BBzNWf4G5NM9yvboC7de2wNzUftPWM2K/BWKbcROOt8/UXb283XZsA74XEgoVHwBuBmvZ5cgw5cIelQ4bIlYD/2xMK%2bmiLgVBZxWeEnuEqHT3DkGEjDXbzBEM34f6Z%2bsZv7zSdODnSwtIq3Npu9j7dUWP8cP8YDlNctSZfik7HxZ4cP/yT7PS/6ACU1XoaQjUyq8BoqgOfSKoNF6GWnkC%2bGDlY//Hiup9TMiJo8h/2823gyJnL%2bD3zKeC3E8qqanLVeXv8RMGMD5wFMz5yEUxzmivQ0DMmYPHtPRYKPl0dJggMCRNsjTgssLJ3koOoNFQZp3g9n%2bLwhSpDBCrqfHwVDBEoDRloBfj3NUak27PEoeq8QRqkQRqkQRokxaF/ADMM39isTnWpAAAAAElFTkSuQmCC' /%3e%3c/svg%3e" width="180" data-src="eleijonmarck/assets/static/unicorn.e6b6009.9ea40e1.gif" data-srcset="eleijonmarck/assets/static/unicorn.e6b6009.9ea40e1.gif 180w" data-sizes="(max-width: 180px) 100vw, 180px" class="author__image g-image g-image--lazy g-image--loading"><noscript><img src="eleijonmarck/assets/static/unicorn.e6b6009.9ea40e1.gif" class="author__image g-image g-image--loaded" width="180" alt="Author image"></noscript><!----><p class="author__intro">
			data ・ ml ・ philosophy
	</p><p class="author__links"><a href="https://github.com/eleijonmarck"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="svg-inline--fa fa-github fa-w-16"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
			Github
		</a><a href="https://twitter.com/eleijonmarck"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="twitter" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-twitter fa-w-16"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg>Twitter
		</a></p></div></main><footer class="footer"><span class="footer__copyright">Copyright © Eric Leijonmarck 2019. </span><span class="footer__links">Powered by <a href="//gridsome.org"> Vue, Gridsome </a></span></footer></div>
    <script src="eleijonmarck/assets/js/app.f762706c.js" defer></script><script src="eleijonmarck/assets/js/component--post.60537bb2.js" defer></script>
  </body>
</html>